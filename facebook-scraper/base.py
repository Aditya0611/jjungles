#!/usr/bin/env python3
"""
Facebook Scraper - Production-Ready Playwright Adapter
=======================================================

A deterministic, maintainable scraper with:
- Fail-fast dependency management (no in-process pip install)
- Unified canonical base classes (BaseScraper â†’ FacebookScraper)
- Externalized config (config/categories.json)
- JSON structured logging with observable metrics
- Retry/backoff decorators for resilience
- Enhanced analytics: time-weighted, sentiment-weighted, normalized
- Unified TrendRecord schema for cross-platform consistency
- Rotating proxy support via ProxyManager
- Lifecycle tracking (first_seen, last_seen, version_id)

Architecture:
-------------
1. DATA MODELS - TrendRecord (unified schema), Platform enum
2. RETRY DECORATORS - retry_page_load, retry_supabase_write
3. PROXY MANAGER - Rotating proxy list with failure handling
4. BASE SCRAPER - Browser management, logging, utilities
5. FACEBOOK SCRAPER - Platform-specific implementation

Usage:
------
    with FacebookScraper(headless=False, debug=True) as scraper:
        if scraper.login():
            results = scraper.get_top_10_trending('technology', max_posts=30)
            scraper.save_results(results, 'technology', 'v1.0')
"""

import os
import sys
import json
import re
import time
import math
import random
import hashlib
import uuid
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from collections import Counter
from functools import wraps
from pathlib import Path
from dataclasses import dataclass, field, asdict
from enum import Enum

# Fail-fast imports - no pip install calls
try:
    from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
    from supabase import create_client, Client
    from textblob import TextBlob
    from dotenv import load_dotenv
    from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
    from pythonjsonlogger import jsonlogger
except ImportError as e:
    sys.stderr.write(f"ERROR: Missing required dependency: {e}\n")
    sys.stderr.write("Please install dependencies: pip install -r requirements.txt\n")
    sys.exit(1)

import logging

load_dotenv()

# Try to import language detection (optional but recommended)
try:
    from langdetect import detect as langdetect_detect, detect_langs, DetectorFactory
    DetectorFactory.seed = 0  # For consistent results
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False
    # Fallback: define placeholder functions
    def langdetect_detect(text):
        return 'en'
    def detect_langs(text):
        return []

# Try to import enhanced sentiment analyzer (optional)
try:
    from sentiment_analyzer import SentimentAnalyzer, SentimentMethod, get_analyzer
    ENHANCED_SENTIMENT_AVAILABLE = True
except ImportError:
    ENHANCED_SENTIMENT_AVAILABLE = False
    # Define placeholder classes if not available
    class SentimentMethod:
        TEXTBLOB = "textblob"
        AUTO = "auto"
    def get_analyzer(*args, **kwargs):
        return None


# ============================================================================
# DATA MODELS - Unified Schema
# ============================================================================

class Platform(Enum):
    """Supported platforms"""
    FACEBOOK = "Facebook"
    INSTAGRAM = "Instagram"
    TWITTER = "Twitter"


@dataclass
class TrendRecord:
    """
    Unified trend record schema for all platforms.
    Provides consistent data structure for storage and analysis.
    """
    platform: str
    topic_hashtag: str
    engagement_score: float
    trending_score: float
    sentiment_polarity: float
    sentiment_label: str
    post_count: int
    total_engagement: int
    avg_engagement: float
    virality_score: float = 0.0  # Virality score (0.0-1.0)
    
    # Detailed metrics
    likes: int = 0
    comments: int = 0
    shares: int = 0
    views: int = 0
    avg_likes: float = 0.0
    avg_comments: float = 0.0
    avg_shares: float = 0.0
    
    # Metadata
    category: str = ""
    hashtag_url: str = ""
    language: str = "en"
    
    # Lifecycle tracking
    version_id: str = ""
    first_seen: Optional[datetime] = None
    last_seen: Optional[datetime] = None
    scraped_at: datetime = field(default_factory=datetime.now)
    
    # Quality indicators
    is_estimated: bool = False
    confidence_score: float = 1.0
    
    # Raw data blob for debugging
    raw_metadata: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        """Convert to dictionary, handling datetime serialization"""
        data = asdict(self)
        for key, value in data.items():
            if isinstance(value, datetime):
                data[key] = value.isoformat() if value else None
        return data
    
    def to_supabase_record(self) -> Dict:
        """Convert to Supabase-compatible format matching actual schema"""
        import uuid
        
        # Convert version_id string to UUID if it's a valid UUID string
        version_id = self.version_id
        try:
            # Try to parse as UUID, if it's already a valid UUID string
            uuid.UUID(version_id)
        except (ValueError, AttributeError):
            # If not a valid UUID, generate one from the string
            version_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(version_id)))
        
        return {
            'platform': self.platform,
            'topic_hashtag': self.topic_hashtag,
            'engagement_score': float(self.engagement_score) if self.engagement_score is not None else None,
            'sentiment_polarity': float(self.sentiment_polarity) if self.sentiment_polarity is not None else None,
            'sentiment_label': self.sentiment_label if self.sentiment_label else None,
            'posts': int(self.post_count) if self.post_count is not None else None,
            'views': int(self.total_engagement) if self.total_engagement is not None else None,
            'version_id': version_id,  # UUID type in Supabase
            'scraped_at': self.scraped_at.isoformat() if self.scraped_at else None,
            'metadata': {
                'category': self.category,
                'trending_score': self.trending_score,
                'avg_engagement': self.avg_engagement,
                'likes': self.likes,
                'comments': self.comments,
                'shares': self.shares,
                'views': self.views,
                'avg_likes': self.avg_likes,
                'avg_comments': self.avg_comments,
                'avg_shares': self.avg_shares,
                'hashtag_url': self.hashtag_url,
                'language': self.language,
                'is_estimated': self.is_estimated,
                'confidence_score': self.confidence_score,
                'first_seen': self.first_seen.isoformat() if self.first_seen else None,
                'last_seen': self.last_seen.isoformat() if self.last_seen else None,
                **self.raw_metadata
            }
        }


# ============================================================================
# RETRY DECORATORS
# ============================================================================

# Retry decorator for page loads
def retry_page_load(max_attempts=3):
    """Retry decorator for page navigation with exponential backoff"""
    return retry(
        stop=stop_after_attempt(max_attempts),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((PlaywrightTimeout, Exception)),
        reraise=True
    )


# Retry decorator for Supabase operations
def retry_supabase_write(max_attempts=3):
    """Retry decorator for Supabase writes with exponential backoff"""
    return retry(
        stop=stop_after_attempt(max_attempts),
        wait=wait_exponential(multiplier=1, min=1, max=5),
        reraise=True
    )


# Retry decorator for element operations (wait_for_selector, click, etc.)
def retry_element_operation(max_attempts=3):
    """Retry decorator for element operations with exponential backoff"""
    return retry(
        stop=stop_after_attempt(max_attempts),
        wait=wait_exponential(multiplier=1, min=1, max=5),
        retry=retry_if_exception_type((PlaywrightTimeout, Exception)),
        reraise=True
    )


# Retry decorator for post extraction operations
def retry_extraction(max_attempts=2):
    """Retry decorator for post extraction with shorter backoff"""
    return retry(
        stop=stop_after_attempt(max_attempts),
        wait=wait_exponential(multiplier=1, min=1, max=3),
        retry=retry_if_exception_type((PlaywrightTimeout, Exception)),
        reraise=True
    )


# ============================================================================
# PROXY MANAGER - Rotating Proxies
# ============================================================================

class ProxyManager:
    """
    Manages rotating proxy list for browser contexts.
    Supports proxy rotation to avoid rate limiting and detection.
    """
    
    def __init__(self, proxy_list: Optional[List[str]] = None):
        """
        Initialize proxy manager.
        
        Args:
            proxy_list: List of proxy URLs (e.g., ['http://proxy1:8080', 'http://proxy2:8080'])
        """
        self.proxies = proxy_list or []
        self.current_index = 0
        self.failed_proxies = set()
    
    def get_next_proxy(self) -> Optional[Dict]:
        """
        Get next available proxy in rotation.
        
        Returns:
            Proxy configuration dict or None if no proxies available
        """
        if not self.proxies:
            return None
        
        available = [p for i, p in enumerate(self.proxies) if i not in self.failed_proxies]
        
        if not available:
            # Reset failed proxies and try again
            self.failed_proxies.clear()
            available = self.proxies
        
        proxy_url = available[self.current_index % len(available)]
        self.current_index += 1
        
        return {'server': proxy_url}
    
    def mark_failed(self, proxy_url: str):
        """Mark proxy as failed"""
        for i, p in enumerate(self.proxies):
            if p == proxy_url:
                self.failed_proxies.add(i)
                break
    
    @staticmethod
    def from_env() -> 'ProxyManager':
        """
        Create ProxyManager from environment variable or local file.
        1. Checks PROXIES env var
        2. Falls back to config/proxies.txt
        """
        proxy_string = os.getenv('PROXIES', '')
        proxies = []
        
        if proxy_string:
            proxies = [p.strip() for p in proxy_string.split(',') if p.strip()]
        
        # Fallback to config/proxies.txt
        if not proxies:
            proxy_file = Path(__file__).parent / "config" / "proxies.txt"
            if proxy_file.exists():
                try:
                    with open(proxy_file, 'r', encoding='utf-8') as f:
                        proxies = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                except Exception as e:
                    print(f"Error reading proxies.txt: {e}")
        
        if not proxies:
            print("WARNING: No proxies found in PROXIES env var or config/proxies.txt")
            print("Scraper will fail unless proxies are provided.")
            
        return ProxyManager(proxies)


class BaseScraper:
    """
    Base scraper with browser management, logging, and utility methods.
    Provides foundation for platform-specific scrapers.
    """
    
    def __init__(self, headless: bool = False, debug: bool = False, proxy_manager: Optional[ProxyManager] = None):
        """Initialize base scraper with browser and logging setup."""
        self.headless = headless
        self.debug = debug
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.seen_text_hashes = set()
        
        # Proxy management
        self.proxy_manager = proxy_manager or ProxyManager.from_env()
        self.current_proxy = None
        self.proxy_requests_count = 0
        self.proxy_session_start = None
        self.proxy_rotation_config = {
            'max_requests_per_proxy': 50,  # Rotate after N requests
            'max_time_per_proxy': 1800,     # Rotate after 30 minutes
            'rotate_on_failure': True,      # Rotate on proxy failure
            'rotate_between_hashtags': True # Rotate between hashtag searches
        }
        
        # Session management
        self.session_dir = Path(__file__).parent / 'sessions'
        self.session_dir.mkdir(exist_ok=True)
        self.login_attempts = 0
        self.last_login_failure_time = None
        self.login_cooldown_until = None
        
        # Load categories from config
        self.categories = self._load_categories()
        
        # Setup JSON logging
        self.logger = self._setup_logging(debug)
        
        self.logger.info("BaseScraper initialized", extra={
            'headless': headless,
            'debug': debug,
            'categories_loaded': len(self.categories),
            'proxies_available': len(self.proxy_manager.proxies)
        })
        
        # Run tracking (Sprint-1 requirement)
        self.platform = "Generic"  # Should be overridden by subclasses
        self.run_id = str(uuid.uuid4())
        self.items_scraped = 0
        self._run_started = False
    
    def _load_categories(self) -> Dict:
        """Load category mappings from config/categories.json"""
        config_path = Path(__file__).parent / 'config' / 'categories.json'
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                categories = json.load(f)
            return categories
        except FileNotFoundError:
            sys.stderr.write(f"ERROR: Config file not found at {config_path}\n")
            sys.exit(1)
        except json.JSONDecodeError as e:
            sys.stderr.write(f"ERROR: Invalid JSON in config file: {e}\n")
            sys.exit(1)
    
    def _setup_logging(self, debug: bool) -> logging.Logger:
        """Setup JSON formatter for structured logging"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG if debug else logging.INFO)
        
        # Remove existing handlers
        logger.handlers.clear()
        
        # JSON formatter
        json_formatter = jsonlogger.JsonFormatter(
            '%(asctime)s %(name)s %(levelname)s %(message)s',
            timestamp=True
        )
        
        # File handler with JSON
        log_file = Path(__file__).parent / 'logs' / 'scraper.log'
        log_file.parent.mkdir(exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(json_formatter)
        logger.addHandler(file_handler)
        
        # Console handler with JSON for production
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(json_formatter)
        logger.addHandler(console_handler)
        
        return logger
    
    def __enter__(self):
        """Context manager entry"""
        self.setup_browser()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.cleanup()
    
    def setup_browser(self):
        """Setup Playwright browser with anti-detection measures and proxy support"""
        try:
            self.logger.info("Setting up browser")
            self.playwright = sync_playwright().start()
            
            self.browser = self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-web-security',
                    '--disable-features=IsolateOrigins,site-per-process'
                ]
            )
            
            # Get proxy configuration
            proxy_config = self.proxy_manager.get_next_proxy()
            
            # Mandatory Proxy Enforcement - "No Bypass" Mandate
            if not proxy_config:
                self.logger.critical("PROXY LIST IS EMPTY OR ALL FAILED. Application cannot continue under 'No Bypass' policy.")
                sys.exit(1)
                
            self.current_proxy = proxy_config['server'] if proxy_config else None
            self.proxy_requests_count = 0
            self.proxy_session_start = time.time()
            
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'locale': 'en-US',
                'timezone_id': 'America/New_York'
            }
            
            # Add proxy if available
            if proxy_config:
                context_options['proxy'] = proxy_config
                # Extract partial proxy for safe logging (e.g., http://u:p@host:port -> http://***@host:port or just host:port)
                p_url = proxy_config['server']
                safe_proxy = p_url.split('@')[-1] if '@' in p_url else p_url
                self.logger.info(f"Using proxy rotation: {safe_proxy}", extra={'proxy': safe_proxy})
            
            # Initialize run tracking in DB if not already started
            if not self._run_started:
                self._start_scrape_run()
            
            self.context = self.browser.new_context(**context_options)
            # Store proxy info in context for tracking
            if proxy_config:
                self.context._proxy = proxy_config
            
            # Anti-detection script
            self.context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                window.chrome = { runtime: {} };
            """)
            
            self.context.set_default_timeout(30000)
            self.page = self.context.new_page()
            
            # Warm up browser
            self.page.goto("https://www.google.com", wait_until="domcontentloaded")
            time.sleep(2)
            
            self.logger.info("Browser setup complete", extra={'proxy_enabled': proxy_config is not None})
            return True
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error("Browser setup failed", extra={'error': error_msg})
            
            # Explicit Proxy Error Classification
            proxy_errors = {
                "407": "PROXY_AUTHENTICATION_FAILED",
                "ERR_TUNNEL_CONNECTION_FAILED": "PROXY_TUNNEL_FAILED",
                "ERR_PROXY_CONNECTION_FAILED": "PROXY_CONNECTION_FAILED",
                "ERR_CONNECTION_RESET": "CONNECTION_RESET_BY_PEER",
                "net::ERR_TIMED_OUT": "PROXY_TIMEOUT"
            }
            
            classified_error = None
            for key, val in proxy_errors.items():
                if key in error_msg:
                    classified_error = val
                    break
            
            strict_mode = os.getenv('PROXY_STRICT_MODE', 'true').lower() == 'true'
            if strict_mode:
                if classified_error:
                    self.logger.critical(f"FATAL PROXY ERROR: {classified_error}")
                    raise RuntimeError(f"FATAL_{classified_error}: {error_msg}")
                self.logger.critical("STRICT MODE: Raising fatal error for browser setup failure")
                raise e
            return False

    def rotate_proxy(self):
        """Force a browser restart with a new proxy for rotation proof"""
        self.logger.info("ðŸ”„ Rotating proxy as requested...")
        self.cleanup()
        return self.setup_browser()
    
    def _should_rotate_proxy(self) -> bool:
        """
        Check if proxy should be rotated based on usage metrics.
        
        Returns:
            True if proxy should be rotated
        """
        if not self.current_proxy or not self.proxy_manager.proxies:
            return False
        
        config = self.proxy_rotation_config
        
        # Check request count
        if self.proxy_requests_count >= config['max_requests_per_proxy']:
            self.logger.info(
                "Proxy rotation needed: max requests reached",
                extra={
                    'current_proxy': self.current_proxy,
                    'requests': self.proxy_requests_count,
                    'max_requests': config['max_requests_per_proxy']
                }
            )
            return True
        
        # Check time-based rotation
        if self.proxy_session_start:
            elapsed_time = time.time() - self.proxy_session_start
            if elapsed_time >= config['max_time_per_proxy']:
                self.logger.info(
                    "Proxy rotation needed: max time reached",
                    extra={
                        'current_proxy': self.current_proxy,
                        'elapsed_seconds': elapsed_time,
                        'max_seconds': config['max_time_per_proxy']
                    }
                )
                return True
        
        return False
    
    def _rotate_proxy(self) -> bool:
        """
        Rotate to next proxy by creating a new browser context.
        Implements bounded retries and hard failure for reliability.
        """
        max_rotation_attempts = 3
        for attempt in range(1, max_rotation_attempts + 1):
            old_proxy = self.current_proxy
            try:
                self.logger.info(f"Rotating proxy (attempt {attempt}/{max_rotation_attempts})")
                
                # Cleanup existing context
                if self.context:
                    try:
                        self.context.close()
                    except:
                        pass
                
                # Get next proxy
                proxy_config = self.proxy_manager.get_next_proxy()
                if not proxy_config:
                    self.logger.critical("No proxies available for rotation!")
                    raise RuntimeError("Proxy rotation failed: No proxies available")
                
                self.current_proxy = proxy_config['server']
                self.proxy_requests_count = 0
                self.proxy_session_start = time.time()
                
                # Create new context with new proxy
                context_options = {
                    'viewport': {'width': 1920, 'height': 1080},
                    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'locale': 'en-US',
                    'timezone_id': 'America/New_York'
                }
                
                if proxy_config:
                    context_options['proxy'] = proxy_config
                
                self.context = self.browser.new_context(**context_options)
                
                # Re-add anti-detection script
                self.context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    window.chrome = { runtime: {} };
                """)
                
                self.context.set_default_timeout(30000)
                self.page = self.context.new_page()
                
                # Re-login if needed (context lost cookies)
                if not self.is_logged_in():
                    self.logger.info("Session lost after proxy rotation, re-logging in")
                    if not self.ensure_logged_in():
                         self.logger.warning(f"Failed to re-login on attempt {attempt}")
                         if attempt == max_rotation_attempts:
                             raise RuntimeError("Failed to re-login after max proxy rotation attempts")
                         continue # Try next proxy
                
                self.logger.info(
                    "Proxy rotated successfully",
                    extra={
                        'old_proxy': old_proxy,
                        'new_proxy': self.current_proxy
                    }
                )
                return True
                
            except Exception as e:
                self.logger.error(f"Error during proxy rotation attempt {attempt}: {e}")
                if attempt == max_rotation_attempts:
                    self.logger.critical("MAX PROXY ROTATION ATTEMPTS REACHED. Hard failing.")
                    raise RuntimeError(f"Proxy rotation failed after {max_rotation_attempts} attempts: {e}")
                
                # Wait before next attempt
                time.sleep(2 * attempt)
        
        return False
    
    def _increment_proxy_requests(self):
        """Increment proxy request counter"""
        self.proxy_requests_count += 1
    
    def _handle_proxy_failure(self):
        """Handle proxy failure by marking it and rotating if configured"""
        if not self.current_proxy:
            return
        
        self.logger.warning(
            "Proxy failure detected",
            extra={'failed_proxy': self.current_proxy}
        )
        
        # Mark proxy as failed
        self.proxy_manager.mark_failed(self.current_proxy)
        
        # Rotate if configured
        if self.proxy_rotation_config.get('rotate_on_failure', False):
            self._rotate_proxy()
    
    def cleanup(self):
        """Clean up browser resources"""
        try:
            # Mark run as completed
            if self._run_started:
                self._update_run_status("completed")
            
            if self.page:
                self.page.close()
            if self.context:
                self.context.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
            self.logger.info("Cleanup completed")
        except Exception as e:
            self.logger.warning("Cleanup error", extra={'error': str(e)})
            if self._run_started:
                try:
                    self._update_run_status("failed", error_message=str(e))
                except:
                    pass

    def _start_scrape_run(self):
        """Initialize scrape run in Supabase (Sprint-1 Compliance)"""
        try:
            url = os.getenv('SUPABASE_URL')
            key = os.getenv('SUPABASE_ANON_KEY')
            if not url or not key:
                return
                
            supabase = create_client(url, key)
            supabase.table('scrape_runs').insert({
                'id': self.run_id,
                'platform': getattr(self, 'platform', 'Unknown'),
                'status': 'running',
                'metadata': {
                    'headless': self.headless,
                    'debug': self.debug,
                    'start_time': datetime.now().isoformat()
                }
            }).execute()
            self._run_started = True
            self.logger.info("Scrape run initialized in DB", extra={'run_id': self.run_id})
        except Exception as e:
            self.logger.warning("Failed to initialize scrape run in DB", extra={'error': str(e)})

    def _update_run_status(self, status: str, error_message: Optional[str] = None):
        """Update scrape run status in Supabase"""
        try:
            url = os.getenv('SUPABASE_URL')
            key = os.getenv('SUPABASE_ANON_KEY')
            if not url or not key:
                return
                
            supabase = create_client(url, key)
            data = {
                'status': status,
                'end_time': datetime.now().isoformat(),
                'items_scraped': self.items_scraped
            }
            if error_message:
                data['error_message'] = error_message
                
            supabase.table('scrape_runs').update(data).eq('id', self.run_id).execute()
        except Exception as e:
            self.logger.debug("Failed to update run status in DB", extra={'error': str(e)})

    def _db_log(self, level: str, message: str, metadata: Optional[Dict] = None):
        """Add log entry to Supabase scraping_log (Sprint-1 Compliance)"""
        try:
            url = os.getenv('SUPABASE_URL')
            key = os.getenv('SUPABASE_ANON_KEY')
            if not url or not key:
                return
                
            supabase = create_client(url, key)
            supabase.table('scraping_log').insert({
                'run_id': self.run_id,
                'level': level,
                'message': message,
                'metadata': metadata or {}
            }).execute()
        except Exception:
            pass # Never let logging crashes break the scraper
    
    @retry_page_load(max_attempts=3)
    def navigate_to(self, url: str, wait_until: str = "load"):
        """Navigate to URL with retry logic and session verification"""
        self.logger.debug("Navigating to URL", extra={'url': url})
        self.page.goto(url, wait_until=wait_until, timeout=45000)
        time.sleep(random.uniform(3, 5))  # Longer wait for page to stabilize
        
        # Verify session is still valid after navigation
        if not self.is_logged_in():
            self.logger.warning("Session lost after navigation", extra={'url': url})
    
    @retry_element_operation(max_attempts=3)
    def _wait_for_selector_safe(self, selector: str, timeout: int = 10000, state: str = 'visible'):
        """
        Wait for selector with retry logic.
        
        Args:
            selector: CSS selector
            timeout: Timeout in milliseconds
            state: Element state ('visible', 'attached', etc.)
        """
        return self.page.wait_for_selector(selector, timeout=timeout, state=state)
    
    @retry_element_operation(max_attempts=3)
    def _click_safe(self, locator, timeout: int = 30000):
        """
        Click element with retry logic.
        
        Args:
            locator: Playwright locator
            timeout: Timeout in milliseconds
        """
        return locator.click(timeout=timeout)
    
    def generate_text_hash(self, text: str) -> str:
        """Generate hash for deduplication"""
        return hashlib.md5(text[:200].encode()).hexdigest()
    
    def detect_language(self, text: str, min_confidence: float = 0.3) -> Tuple[str, float]:
        """
        Detect language of text using langdetect.
        
        Args:
            text: Text to analyze
            min_confidence: Minimum confidence threshold (0.0 to 1.0)
            
        Returns:
            Tuple of (language_code, confidence)
            - language_code: ISO 639-1 language code (e.g., 'en', 'es', 'fr')
            - confidence: Detection confidence (0.0 to 1.0)
        """
        if not text or len(text.strip()) < 10:
            return 'en', 0.0  # Default to English for very short text
        
        if not LANGDETECT_AVAILABLE:
            return 'en', 0.5  # Default if langdetect not available
        
        try:
            # Detect primary language
            detected_lang = langdetect_detect(text)
            
            # Get confidence score from multiple language probabilities
            try:
                languages = detect_langs(text)
                if languages:
                    # Confidence is the probability of the top language
                    confidence = languages[0].prob
                    detected_lang = languages[0].lang
                    
                    # If confidence is below threshold, default to 'en'
                    if confidence < min_confidence:
                        return 'en', confidence
                    
                    return detected_lang, round(confidence, 3)
                else:
                    return detected_lang, 0.5
            except:
                # Fallback if detect_langs fails
                return detected_lang, 0.7
            
        except Exception as e:
            self.logger.debug(f"Language detection failed: {e}", extra={'text_preview': text[:50]})
            return 'en', 0.0  # Default to English on error
    
    def analyze_sentiment(self, text: str, use_enhanced: bool = False) -> Tuple[str, float, float]:
        """
        Analyze sentiment using TextBlob or enhanced analyzer.
        
        Args:
            text: Text to analyze
            use_enhanced: If True, use enhanced sentiment analyzer (HuggingFace if available)
            
        Returns:
            Tuple of (sentiment_label, polarity_score, subjectivity/confidence)
        """
        # Try enhanced analyzer if available and requested
        if use_enhanced and ENHANCED_SENTIMENT_AVAILABLE:
            try:
                from sentiment_analyzer import get_analyzer, SentimentMethod
                analyzer = get_analyzer(SentimentMethod.AUTO)
                sentiment, polarity, confidence = analyzer.analyze(text)
                # Return format: (sentiment, polarity, confidence/subjectivity)
                subjectivity = confidence if confidence is not None else abs(polarity)
                return sentiment, polarity, round(subjectivity, 3)
            except Exception as e:
                self.logger.debug("Enhanced sentiment analysis failed, falling back to TextBlob", 
                                extra={'error': str(e)})
        
        # Fallback to TextBlob (default)
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity
            
            if polarity > 0.1:
                sentiment = "positive"
            elif polarity < -0.1:
                sentiment = "negative"
            else:
                sentiment = "neutral"
            
            return sentiment, round(polarity, 3), round(subjectivity, 3)
        except Exception as e:
            self.logger.debug("Sentiment analysis failed", extra={'error': str(e)})
            return "neutral", 0.0, 0.0
    
    def is_brand_safe(self, text: str, min_sentiment: float = 0.0) -> bool:
        """
        Check if text is brand-safe (non-negative sentiment).
        
        Args:
            text: Text to check
            min_sentiment: Minimum sentiment score threshold (default: 0.0 = neutral+)
            
        Returns:
            True if brand-safe, False otherwise
        """
        if ENHANCED_SENTIMENT_AVAILABLE:
            try:
                from sentiment_analyzer import get_analyzer, SentimentMethod
                analyzer = get_analyzer(SentimentMethod.AUTO)
                return analyzer.is_brand_safe(text, min_sentiment)
            except Exception:
                pass  # Fallback to simple check
        
        sentiment, polarity, _ = self.analyze_sentiment(text)
        return polarity >= min_sentiment and sentiment != "negative"

    def _normalize_to_trend_records(
        self, 
        results: List[Dict], 
        category: str, 
        version_id: str
    ) -> List[TrendRecord]:
        """
        Normalize hashtag dictionaries to unified TrendRecord schema.
        
        Args:
            results: List of hashtag dictionaries
            category: Category name
            version_id: Version identifier
            
        Returns:
            List of TrendRecord objects
        """
        records = []
        now = datetime.now()
        
        for item in results:
            record = TrendRecord(
                platform=getattr(self, 'platform', Platform.FACEBOOK.value),
                topic_hashtag=item['hashtag'],
                engagement_score=float(item['engagement_score']),
                trending_score=float(item.get('trending_score', 0)),
                virality_score=float(item.get('virality_score', 0.0)),
                sentiment_polarity=float(item.get('sentiment_score', 0)),
                sentiment_label=item['sentiment'],
                post_count=int(item['post_count']),
                total_engagement=int(item['total_engagement']),
                avg_engagement=float(item['avg_engagement']),
                
                # Detailed metrics
                likes=int(item.get('likes', 0)),
                comments=int(item.get('comments', 0)),
                shares=int(item.get('shares', 0)),
                views=int(item['total_engagement']),  
                avg_likes=float(item.get('avg_likes', 0)),
                avg_comments=float(item.get('avg_comments', 0)),
                avg_shares=float(item.get('avg_shares', 0)),
                
                # Metadata
                category=category,
                hashtag_url=item['hashtag_url'],
                language=item.get('primary_language', 'en'),  # Use detected primary language
                
                # Lifecycle
                version_id=version_id,
                first_seen=now,
                last_seen=now,
                scraped_at=now,
                
                # Quality
                is_estimated=item.get('is_estimated', False),
                confidence_score=0.8 if item.get('is_estimated') else 1.0,
                
                # Raw blob for debugging (includes post type and media info)
                raw_metadata={
                    'source': 'facebook_scraper',
                    'original': item,
                    'post_types': item.get('post_types', {}),
                    'media_stats': item.get('media_stats', {}),
                    'media_samples': item.get('media_samples', {}),
                    'language_distribution': item.get('language_distribution', {}),
                    'avg_language_confidence': item.get('avg_language_confidence', 0.0)
                }
            )
            records.append(record)
        
        self.logger.debug(
            "Normalized to TrendRecords",
            extra={'count': len(records), 'version_id': version_id}
        )
        
        return records

    def save_results(self, results: List[Dict], category: str, version_id: str):
        """
        Save results to JSON file and Supabase (with retry logic).
        Uses unified TrendRecord schema for consistency.
        
        Args:
            results: List of hashtag dictionaries
            category: Category name
            version_id: Version identifier
        """
        # Normalize to unified schema
        trend_records = self._normalize_to_trend_records(results, category, version_id)
        
        # Save to JSON file (as dicts)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        data_dir = Path(__file__).parent / 'data'
        data_dir.mkdir(exist_ok=True)
        filename = data_dir / f"facebook_top10_{category}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump([r.to_dict() for r in trend_records], f, indent=2, ensure_ascii=False)
        
        # Increment items scraped count for run tracking
        self.items_scraped += len(trend_records)
        
        self.logger.info("Results saved to file", extra={'output_file': str(filename)})
        
        # Save to Supabase with retry
        try:
            self._save_to_supabase_normalized(trend_records, version_id)
        except Exception as e:
            self.logger.error("Supabase save failed after retries", extra={'error': str(e)})
            # Always fail hard on DB error in production logic
            self.logger.critical("FATAL: Supabase insertion failed. Crashing to prevent data loss.")
            sys.exit(1)

    @retry_supabase_write(max_attempts=3)
    def _save_to_supabase_normalized(self, trend_records: List[TrendRecord], version_id: str):
        """
        Save normalized TrendRecords to Supabase with retry logic.
        Uses unified schema for consistent cross-platform storage.
        Implements lifecycle tracking: preserves first_seen, updates last_seen.
        
        Args:
            trend_records: List of TrendRecord objects
            version_id: Version identifier
        """
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_ANON_KEY')
        
        if not url or not key:
            self.logger.critical("Supabase credentials not found. DB insertion is mandatory.")
            raise ValueError("Missing SUPABASE_URL or SUPABASE_ANON_KEY. Cannot persist data.")
        
        supabase: Client = create_client(url, key)
        
        # Lifecycle tracking: Query existing records to preserve first_seen
        now = datetime.now()
        hashtag_to_earliest_first_seen = {}
        
        # Get unique hashtags from current batch
        unique_hashtags = list(set(record.topic_hashtag for record in trend_records))
        
        try:
            if unique_hashtags:
                # Query existing records for these hashtags
                # We'll query in batches to avoid URL length issues
                batch_size = 50
                for i in range(0, len(unique_hashtags), batch_size):
                    batch = unique_hashtags[i:i + batch_size]
                    
                    # Query all existing records for these hashtags
                    query = supabase.table('facebook').select('topic_hashtag, metadata').in_('topic_hashtag', batch)
                    existing_records = query.execute()
                    
                    # Extract earliest first_seen for each hashtag
                    for record in existing_records.data:
                        hashtag = record.get('topic_hashtag')
                        metadata = record.get('metadata', {})
                        
                        if hashtag and metadata:
                            first_seen_str = metadata.get('first_seen')
                            if first_seen_str:
                                try:
                                    # Parse ISO format datetime - handle various formats
                                    first_seen_str_clean = first_seen_str.replace('Z', '+00:00')
                                    # Remove microseconds if present and causing issues
                                    if '.' in first_seen_str_clean and '+' in first_seen_str_clean:
                                        parts = first_seen_str_clean.split('+')
                                        if '.' in parts[0]:
                                            # Keep only up to seconds, remove microseconds
                                            time_part = parts[0].split('.')[0]
                                            first_seen_str_clean = time_part + '+' + parts[1]
                                    
                                    first_seen_dt = datetime.fromisoformat(first_seen_str_clean)
                                    
                                    # Track earliest first_seen for each hashtag
                                    if hashtag not in hashtag_to_earliest_first_seen:
                                        hashtag_to_earliest_first_seen[hashtag] = first_seen_dt
                                    else:
                                        # Keep the earliest one
                                        if first_seen_dt < hashtag_to_earliest_first_seen[hashtag]:
                                            hashtag_to_earliest_first_seen[hashtag] = first_seen_dt
                                except (ValueError, AttributeError, TypeError) as e:
                                    self.logger.debug(
                                        f"Could not parse first_seen for {hashtag}: {first_seen_str}",
                                        extra={'error': str(e)}
                                    )
        except Exception as e:
            self.logger.warning(
                "Error querying existing records for lifecycle tracking, using current timestamps",
                extra={'error': str(e)}
            )
            # Continue with current timestamps if query fails
        
        # Update trend_records with lifecycle tracking
        for record in trend_records:
            hashtag = record.topic_hashtag
            
            # If we found an existing first_seen, preserve it; otherwise use now
            if hashtag in hashtag_to_earliest_first_seen:
                record.first_seen = hashtag_to_earliest_first_seen[hashtag]
                self.logger.debug(
                    f"Preserved first_seen for {hashtag}: {record.first_seen}",
                    extra={'hashtag': hashtag, 'first_seen': record.first_seen.isoformat()}
                )
            else:
                # New trend, set first_seen to now
                record.first_seen = now
                self.logger.debug(
                    f"New trend {hashtag}, setting first_seen to now",
                    extra={'hashtag': hashtag}
                )
            
            # Always update last_seen to current time
            record.last_seen = now
        
        # Convert TrendRecords to Supabase format
        records = [record.to_supabase_record() for record in trend_records]
        
        response = supabase.table('facebook').insert(records).execute()
        
        # Validate response - Supabase might return error in response object without raising Python exception
        if hasattr(response, 'error') and response.error:
            self.logger.error("Supabase insert returned error", extra={'error': response.error})
            raise RuntimeError(f"Supabase Insert Error: {response.error}")
        
        # Log lifecycle tracking stats
        new_trends = sum(1 for h in unique_hashtags if h not in hashtag_to_earliest_first_seen)
        existing_trends = len(unique_hashtags) - new_trends
        
        self.logger.info("Supabase upload successful", extra={
            'records_count': len(records),
            'version_id': version_id,
            'schema': 'unified_trend_record',
            'lifecycle_tracking': {
                'new_trends': new_trends,
                'existing_trends': existing_trends,
                'first_seen_preserved': existing_trends
            }
        })


class FacebookScraper(BaseScraper):
    """
    Facebook-specific scraper implementation.
    Extends BaseScraper with Facebook login and hashtag extraction logic.
    """
    
    def __init__(self, headless: bool = False, debug: bool = False):
        """Initialize Facebook scraper."""
        super().__init__(headless, debug)
        self.platform = Platform.FACEBOOK.value
        
        self.email = os.getenv('FACEBOOK_EMAIL')
        self.password = os.getenv('FACEBOOK_PASSWORD')
        
        if not self.email or not self.password:
            raise ValueError(
                "Facebook credentials not found. "
                "Set FACEBOOK_EMAIL and FACEBOOK_PASSWORD in .env file"
            )
    
    def _get_session_file(self) -> Path:
        """Get path to session cookie file."""
        # Use email hash as session identifier
        email_hash = hashlib.md5(self.email.encode()).hexdigest()[:8]
        return self.session_dir / f"facebook_session_{email_hash}.json"
    
    def _save_cookies(self):
        """Save current browser cookies to session file."""
        try:
            if not self.context:
                return
            
            cookies = self.context.cookies()
            session_file = self._get_session_file()
            
            session_data = {
                'cookies': cookies,
                'saved_at': datetime.now().isoformat(),
                'user_agent': self.context.user_agent if hasattr(self.context, 'user_agent') else None
            }
            
            with open(session_file, 'w') as f:
                json.dump(session_data, f, indent=2)
            
            self.logger.debug("Cookies saved", extra={'session_file': str(session_file)})
        except Exception as e:
            self.logger.debug(f"Failed to save cookies: {e}")
    
    def _load_cookies(self) -> bool:
        """Load cookies from session file and apply to browser context."""
        try:
            session_file = self._get_session_file()
            
            if not session_file.exists():
                return False
            
            with open(session_file, 'r') as f:
                session_data = json.load(f)
            
            cookies = session_data.get('cookies', [])
            if not cookies:
                return False
            
            # Check if session is too old (older than 24 hours)
            saved_at = datetime.fromisoformat(session_data['saved_at'])
            age_hours = (datetime.now() - saved_at).total_seconds() / 3600
            if age_hours > 24:
                self.logger.debug("Session expired (older than 24 hours)", extra={'age_hours': age_hours})
                return False
            
            if self.context:
                self.context.add_cookies(cookies)
                self.logger.debug("Cookies loaded", extra={'cookie_count': len(cookies), 'age_hours': round(age_hours, 1)})
                return True
            
            return False
        except Exception as e:
            self.logger.debug(f"Failed to load cookies: {e}")
            return False
    
    def is_logged_in(self) -> bool:
        """
        Check if currently logged in to Facebook.
        
        Returns:
            bool: True if logged in, False otherwise
        """
        try:
            if not self.page:
                return False
            
            current_url = self.page.url
            page_title = self.page.title().lower()
            
            # Check for login redirects
            if "login" in current_url.lower() or "login" in page_title:
                return False
            
            # Check for privacy_mutation_token (Facebook blocking login)
            if "privacy_mutation_token" in current_url:
                return False
            
            # Check for login page indicators
            try:
                # Check if login form exists
                email_field = self.page.locator('#email').first
                if email_field.is_visible(timeout=2000):
                    return False
            except:
                pass
            
            # If we're on a non-login page, assume logged in
            return True
            
        except Exception as e:
            self.logger.debug(f"Error checking login status: {e}")
            return False
    
    def ensure_logged_in(self) -> bool:
        """
        Ensure we're logged in, re-login if necessary.
        
        Returns:
            bool: True if logged in (or re-login successful), False otherwise
        """
        if self.is_logged_in():
            return True
        
        # Check if we're in cooldown period
        if self.login_cooldown_until and datetime.now() < self.login_cooldown_until:
            remaining = (self.login_cooldown_until - datetime.now()).total_seconds() / 60
            self.logger.warning(
                "Login cooldown active",
                extra={'remaining_minutes': round(remaining, 1)}
            )
            return False
        
        self.logger.warning("Session expired, re-logging in...")
        return self.login()
    
    def login(self) -> bool:
        """
        Login to Facebook with human-like behavior and cookie persistence.
        
        Returns:
            bool: True if login successful
        """
        try:
            # Validate credentials first
            if not self.email or not self.password:
                self.logger.error(
                    "Login failed: Credentials not found",
                    extra={'email_set': bool(self.email), 'password_set': bool(self.password)}
                )
                return False
            
            # Check cooldown period
            if self.login_cooldown_until and datetime.now() < self.login_cooldown_until:
                remaining = (self.login_cooldown_until - datetime.now()).total_seconds() / 60
                self.logger.warning(
                    "Login blocked: Cooldown active",
                    extra={'remaining_minutes': round(remaining, 1)}
                )
                return False
            
            # Exponential backoff for repeated failures
            if self.login_attempts > 0:
                backoff_seconds = min(60 * (2 ** (self.login_attempts - 1)), 300)  # Max 5 minutes
                self.logger.info(
                    "Applying login backoff",
                    extra={'attempts': self.login_attempts, 'backoff_seconds': backoff_seconds}
                )
                time.sleep(backoff_seconds)
            
            # Try loading saved cookies first
            if self._load_cookies():
                self.logger.info("Attempting to use saved session")
                self.navigate_to("https://www.facebook.com")
                time.sleep(5)  # Wait for page to load
                
                # Check if cookies worked
                if self.is_logged_in():
                    self.logger.info("Session restored from cookies")
                    self.login_attempts = 0  # Reset on success
                    return True
                else:
                    self.logger.debug("Saved cookies expired or invalid")
            
            self.login_attempts += 1
            self.logger.info("Starting Facebook login", extra={'attempt': self.login_attempts})
            self.logger.debug(
                "Login attempt",
                extra={'email': self.email[:3] + '***' if self.email else 'None'}
            )
            time.sleep(random.uniform(2, 4))
            
            # Navigate to Facebook
            self.navigate_to("https://www.facebook.com")
            
            # Wait for page to load completely
            try:
                self.page.wait_for_load_state("networkidle", timeout=15000)
            except:
                pass  # Continue even if networkidle times out
            
            time.sleep(2)  # Additional wait for dynamic content
            
            # Handle cookie consent
            try:
                cookie_btn = self.page.locator(
                    "button:has-text('Accept'), button:has-text('Allow all cookies')"
                ).first
                if cookie_btn.is_visible(timeout=3000):
                    self._click_safe(cookie_btn, timeout=5000)
                    time.sleep(2)
            except:
                pass
            
            # Debug: Log current page info
            current_url = self.page.url
            page_title = self.page.title()
            self.logger.debug(
                "Login page loaded",
                extra={'url': current_url, 'title': page_title[:100]}
            )
            
            # Try multiple selector strategies for email field
            email_field = None
            email_selectors = ['#email', 'input[type="email"]', 'input[name="email"]', 'input[placeholder*="email" i]', 'input[placeholder*="phone" i]']
            
            for selector in email_selectors:
                try:
                    locator = self.page.locator(selector).first
                    if locator.is_visible(timeout=5000):
                        email_field = locator
                        self.logger.debug(f"Found email field with selector: {selector}")
                        break
                except:
                    continue
            
            if not email_field:
                # Take screenshot for debugging
                if self.debug:
                    debug_dir = Path("debug")
                    debug_dir.mkdir(exist_ok=True)
                    screenshot_path = debug_dir / f"login_error_{int(time.time())}.png"
                    self.page.screenshot(path=str(screenshot_path))
                    self.logger.debug(f"Debug screenshot saved: {screenshot_path}")
                
                self.logger.error(
                    "Login failed: Email field not found",
                    extra={'url': current_url, 'title': page_title[:100]}
                )
                raise Exception(f"Email field not found on page. URL: {current_url}, Title: {page_title[:100]}")
            
            # Wait for email field to be clickable
            email_field.wait_for(state="visible", timeout=10000)
            self._click_safe(email_field, timeout=10000)
            time.sleep(0.5)
            for char in self.email:
                email_field.type(char, delay=random.uniform(80, 150))
            
            time.sleep(1)
            
            # Fill password with human-like typing - try multiple selectors
            pass_field = None
            pass_selectors = ['#pass', 'input[type="password"]', 'input[name="pass"]']
            
            for selector in pass_selectors:
                try:
                    locator = self.page.locator(selector).first
                    if locator.is_visible(timeout=5000):
                        pass_field = locator
                        self.logger.debug(f"Found password field with selector: {selector}")
                        break
                except:
                    continue
            
            if not pass_field:
                self.logger.error("Login failed: Password field not found")
                raise Exception("Password field not found on page")
            
            pass_field.wait_for(state="visible", timeout=10000)
            self._click_safe(pass_field, timeout=10000)
            time.sleep(0.5)
            for char in self.password:
                pass_field.type(char, delay=random.uniform(80, 150))
            
            time.sleep(1)
            
            # Click login button - handle navigation timeout gracefully
            login_btn = self.page.locator('button[name="login"]').first
            url_before_click = self.page.url
            try:
                # Try clicking with navigation wait (longer timeout)
                login_btn.click(timeout=60000)  # 60 second timeout
                self.logger.debug("Login button clicked - navigation completed")
            except Exception as e:
                # If navigation times out, check if we're on a different page
                url_after_click = self.page.url
                navigation_occurred = url_before_click != url_after_click
                
                self.logger.debug(
                    "Login button click navigation timeout - checking page state",
                    extra={
                        'error': str(e)[:200],
                        'url_before': url_before_click,
                        'url_after': url_after_click,
                        'navigation_occurred': navigation_occurred
                    }
                )
                
                # If URL changed, navigation happened but Playwright didn't detect completion
                if navigation_occurred:
                    self.logger.info(
                        "Navigation occurred despite timeout - continuing",
                        extra={'new_url': url_after_click}
                    )
                else:
                    # No navigation - might be stuck on login page or showing CAPTCHA
                    self.logger.warning(
                        "No navigation after login click - may be CAPTCHA or security check",
                        extra={'url': url_after_click}
                    )
                
                # Wait a bit for page to settle
                time.sleep(5)
            
            # Wait for page to potentially load or show security check
            time.sleep(random.uniform(8, 12))
            
            # Check for CAPTCHA or security challenges before proceeding
            try:
                # Check for common CAPTCHA indicators
                captcha_indicators = [
                    'iframe[src*="captcha"]',
                    'div[id*="captcha"]',
                    'div[class*="captcha"]',
                    'iframe[title*="captcha" i]',
                    'div:has-text("Verify")',
                    'div:has-text("Security Check")'
                ]
                for indicator in captcha_indicators:
                    try:
                        if self.page.locator(indicator).first.is_visible(timeout=3000):
                            self.logger.warning(
                                "CAPTCHA or security challenge detected after login click",
                                extra={'indicator': indicator}
                            )
                            if self.headless:
                                self.logger.error(
                                    "CAPTCHA detected in headless mode - cannot proceed automatically. "
                                    "Please run with headless=False to solve CAPTCHA manually."
                                )
                                return False
                            else:
                                print("\n" + "="*80)
                                print("âš ï¸  CAPTCHA/SECURITY CHECK DETECTED")
                                print("="*80)
                                print("\nPlease complete the CAPTCHA or security check in the browser window.")
                                print("The script will wait up to 5 minutes...")
                                print("="*80 + "\n")
                                
                                # Wait for CAPTCHA to be solved
                                max_wait = 300
                                elapsed = 0
                                while elapsed < max_wait:
                                    time.sleep(5)
                                    elapsed += 5
                                    # Check if CAPTCHA is gone
                                    captcha_found = False
                                    for ind in captcha_indicators:
                                        try:
                                            if self.page.locator(ind).first.is_visible(timeout=1000):
                                                captcha_found = True
                                                break
                                        except:
                                            pass
                                    if not captcha_found:
                                        self.logger.info("CAPTCHA resolved - continuing")
                                        break
                                    if elapsed % 30 == 0:
                                        remaining = (max_wait - elapsed) // 60
                                        print(f"â³ Still waiting for CAPTCHA... ({remaining} minutes remaining)")
                                
                                if elapsed >= max_wait:
                                    self.logger.error("CAPTCHA timeout")
                                    return False
                            break
                    except:
                        continue
            except Exception as e:
                self.logger.debug(f"Error checking for CAPTCHA: {e}")
            
            # Handle "Save Login Info" prompt
            try:
                not_now = self.page.locator(
                    "div:has-text('Not Now'), button:has-text('Not Now')"
                ).first
                if not_now.is_visible(timeout=5000):
                    self._click_safe(not_now, timeout=5000)
                    time.sleep(2)
            except:
                pass
            
            # Check for checkpoint/security verification
            current_url = self.page.url
            if "checkpoint" in current_url.lower():
                self.logger.warning(
                    "Facebook checkpoint detected - manual intervention required",
                    extra={'url': current_url}
                )
                
                if not self.headless:
                    print("\n" + "="*80)
                    print("âš ï¸  FACEBOOK SECURITY CHECKPOINT DETECTED")
                    print("="*80)
                    print("\nFacebook is asking for additional verification.")
                    print("This is common when using automation tools.")
                    print("\nPlease complete the verification in the browser window:")
                    print("  1. Complete any 2FA/security checks")
                    print("  2. Navigate through any security prompts")
                    print("  3. Wait until you reach your Facebook homepage")
                    print("\nThe script will wait up to 5 minutes for you to complete this...")
                    print("="*80 + "\n")
                    
                    # Wait for user to complete checkpoint (check every 5 seconds)
                    max_wait_time = 300  # 5 minutes
                    wait_interval = 5
                    elapsed = 0
                    
                    while elapsed < max_wait_time:
                        time.sleep(wait_interval)
                        elapsed += wait_interval
                        
                        # Check current URL
                        try:
                            current_url = self.page.url
                            if "checkpoint" not in current_url.lower() and "login" not in current_url.lower():
                                # Successfully past checkpoint
                                self.logger.info("Checkpoint resolved - login successful")
                                print("\nâœ“ Checkpoint resolved! Continuing...\n")
                                time.sleep(3)
                                return True
                        except:
                            pass
                        
                        # Show progress every 30 seconds
                        if elapsed % 30 == 0:
                            remaining = (max_wait_time - elapsed) // 60
                            print(f"â³ Still waiting... ({remaining} minutes remaining)")
                    
                    # Timeout
                    print("\nâŒ Timeout: Checkpoint not resolved within 5 minutes")
                    return False
                else:
                    self.logger.error(
                        "Checkpoint detected but headless mode is enabled. "
                        "Please run with headless=False to manually resolve checkpoint."
                    )
                    return False
            
            # Wait a bit more to ensure redirect completes
            time.sleep(3)
            current_url = self.page.url
            
            # Verify login success - check multiple indicators
            if "login" not in current_url.lower() and "facebook.com" in current_url.lower():
                # Additional check - verify we're not on login page by checking for feed
                try:
                    # Check if we can see news feed or home indicators
                    feed_indicators = [
                        '[aria-label*="News Feed"]',
                        '[aria-label*="Home"]',
                        'div[role="feed"]',
                        'div[role="main"]',
                        'a[href="/"]:has-text("Home")'
                    ]
                    logged_in = False
                    for indicator in feed_indicators:
                        try:
                            if self.page.locator(indicator).first.is_visible(timeout=3000):
                                logged_in = True
                                break
                        except:
                            continue
                    
                    if logged_in or "facebook.com" in current_url and "login" not in current_url.lower():
                        self.logger.info("Facebook login successful")
                        # Save cookies for future use
                        self._save_cookies()
                        # Reset login attempts on success
                        self.login_attempts = 0
                        self.login_cooldown_until = None
                        # Wait longer after login to establish session
                        time.sleep(random.uniform(5, 8))
                        return True
                except:
                    # Fallback - if URL doesn't have login, assume success
                    if "login" not in current_url.lower():
                        self.logger.info("Facebook login successful (verified by URL)")
                        # Save cookies for future use
                        self._save_cookies()
                        # Reset login attempts on success
                        self.login_attempts = 0
                        self.login_cooldown_until = None
                        # Wait longer after login to establish session
                        time.sleep(random.uniform(5, 8))
                        return True
            
            # Login failed - redirected to login page
            error_message = ""
            try:
                # Wait a bit for error messages to appear
                time.sleep(2)
                # Try to find common error messages
                error_selectors = [
                    'div[role="alert"]',
                    'div[class*="error"]',
                    'div[id*="error"]',
                    'div:has-text("incorrect")',
                    'div:has-text("wrong")',
                    'div:has-text("try again")',
                    'div:has-text("Invalid")',
                    '[data-testid="error"]'
                ]
                for selector in error_selectors:
                    try:
                        error_elem = self.page.locator(selector).first
                        if error_elem.is_visible(timeout=3000):
                            error_message = error_elem.inner_text()[:200]
                            break
                    except:
                        continue
            except:
                pass
            
            self.logger.error(
                "Login failed - redirected to login page",
                extra={'url': current_url, 'error_message': error_message, 'attempt': self.login_attempts}
            )
            
            # Implement cooldown after multiple failures
            if self.login_attempts >= 3:
                cooldown_minutes = min(30 * (self.login_attempts - 2), 120)  # Max 2 hours
                self.login_cooldown_until = datetime.now().replace(microsecond=0) + timedelta(minutes=cooldown_minutes)
                self.logger.warning(
                    "Too many login failures - entering cooldown period",
                    extra={'cooldown_minutes': cooldown_minutes, 'cooldown_until': self.login_cooldown_until.isoformat()}
                )
            
            # If not headless, allow manual intervention
            if not self.headless:
                print("\n" + "="*80)
                print("âš ï¸  LOGIN FAILED - Manual Intervention Required")
                print("="*80)
                print("\nFacebook redirected back to the login page.")
                if error_message:
                    print(f"\nError detected: {error_message}")
                print("\nPossible reasons:")
                print("  â€¢ Incorrect email or password")
                print("  â€¢ Facebook security check required")
                print("  â€¢ 2FA verification needed")
                print("  â€¢ Account temporarily locked")
                print("\nPlease check the browser window and:")
                print("  1. Verify credentials are correct")
                print("  2. Complete any security checks manually")
                print("  3. If login succeeds, the script will continue automatically")
                print("\nThe script will wait up to 3 minutes for manual login...")
                print("="*80 + "\n")
                
                # Wait for manual login (check every 5 seconds)
                max_wait_time = 180  # 3 minutes
                wait_interval = 5
                elapsed = 0
                
                while elapsed < max_wait_time:
                    time.sleep(wait_interval)
                    elapsed += wait_interval
                    
                    # Check current URL
                    try:
                        current_url = self.page.url
                        if "login" not in current_url.lower() and "checkpoint" not in current_url.lower():
                            # Successfully logged in manually
                            self.logger.info("Manual login successful")
                            print("\nâœ“ Manual login detected! Continuing...\n")
                            time.sleep(3)
                            return True
                    except:
                        pass
                    
                    # Show progress every 30 seconds
                    if elapsed % 30 == 0:
                        remaining = (max_wait_time - elapsed) // 60
                        print(f"â³ Still waiting for manual login... ({remaining} minutes remaining)")
                
                # Timeout
                print("\nâŒ Timeout: Manual login not completed within 3 minutes")
                return False
            
        except Exception as e:
            self.logger.error("Login failed", extra={'error': str(e)})
            return False
    
    def scrape_category_hashtags(self, category: str, max_posts: int = 50) -> List[Dict]:
        """
        Scrape hashtags for a specific category.
        
        Args:
            category: Category name
            max_posts: Maximum posts to process
            
        Returns:
            List of hashtag data dictionaries
        """
        if category.lower() not in self.categories:
            self.logger.error(
                "Unknown category",
                extra={
                    'category': category,
                    'available': list(self.categories.keys())
                }
            )
            return []
        
        cat_data = self.categories[category.lower()]
        search_terms = cat_data['keywords']
        
        self.logger.info(
            "Starting category scrape",
            extra={
                'category': category,
                'search_terms': search_terms[:3],
                'max_posts': max_posts
            }
        )
        
        all_hashtag_data = {}
        
        # Use more keywords for better coverage (5-6 instead of 3)
        keywords_to_use = min(6, len(search_terms))
        posts_per_keyword = max(15, max_posts // keywords_to_use)  # At least 15 posts per keyword
        
        self.logger.info(
            f"Using {keywords_to_use} keywords, targeting {posts_per_keyword} posts per keyword"
        )
        
        # Search each keyword
        for i, keyword in enumerate(search_terms[:keywords_to_use], 1):
            self.logger.info(
                "Searching keyword",
                extra={
                    'keyword': keyword,
                    'progress': f"{i}/{keywords_to_use}",
                    'target_posts': posts_per_keyword
                }
            )
            
            try:
                # Check and refresh login if needed before each keyword
                if not self.ensure_logged_in():
                    self.logger.error(f"Failed to maintain login for keyword: {keyword}")
                    continue
                
                # Strategy 1: Try hashtag page directly (more reliable than search)
                hashtag_keyword = keyword.replace(' ', '').lower()
                hashtag_url = f"https://www.facebook.com/hashtag/{hashtag_keyword}"
                
                self.logger.info(f"Trying hashtag page: {hashtag_url}")
                self.navigate_to(hashtag_url)
                time.sleep(5)  # Wait for page to load
                
                # Check if we got redirected to login
                if not self.is_logged_in():
                    self.logger.warning(f"Redirected to login after navigating to {hashtag_url}, re-logging in...")
                    if not self.ensure_logged_in():
                        self.logger.error(f"Failed to re-login for keyword: {keyword}")
                        # Rotate proxy on login failure if configured
                        if self.proxy_rotation_config.get('rotate_on_failure', False):
                            self._handle_proxy_failure()
                        self.logger.critical("Login session lost and recovery failed")
                        raise RuntimeError("Login session lost and recovery failed - stopping to avoid detection")
                    # Wait longer after re-login before retrying
                    time.sleep(random.uniform(5, 8))
                    # Retry navigation after re-login
                    self.navigate_to(hashtag_url)
                    time.sleep(5)
                
                # Check if proxy rotation is needed before scraping
                if self._should_rotate_proxy():
                    if not self._rotate_proxy():
                        self.logger.critical("CRITICAL: Failed to rotate proxy before scraping")
                        raise RuntimeError("Proxy rotation failed - stopping to avoid detection")
                
                # Try to find posts - use posts_per_keyword instead of max_posts // 3
                posts = self._extract_posts_from_page(posts_per_keyword)
                
                # Increment proxy request counter
                self._increment_proxy_requests()
                
                # Strategy 2: If hashtag page fails, try search
                if not posts:
                    self.logger.info(f"Hashtag page empty, trying search: {keyword}")
                    # Ensure still logged in before search
                    if not self.ensure_logged_in():
                        self.logger.critical("Failed to ensure login state before search")
                        raise RuntimeError("Login check failed - stopping to avoid unauthenticated scraping")
                    
                    # Rotate proxy between hashtag searches - ENFORCED
                    if self.current_proxy:
                        if not self._rotate_proxy():
                            self.logger.error("CRITICAL: Proxy rotation failed between hashtag searches")
                            raise RuntimeError("Proxy rotation failed - stopping to avoid detection")
                    
                    search_url = f"https://www.facebook.com/search/posts?q={keyword}"
                    self.navigate_to(search_url)
                    time.sleep(5)  # Wait for page to load
                    
                    # Check if we got redirected to login
                    if not self.is_logged_in():
                        self.logger.warning(f"Redirected to login after navigating to {search_url}, re-logging in...")
                        if not self.ensure_logged_in():
                            self.logger.error(f"Failed to re-login for keyword: {keyword}")
                            raise RuntimeError(f"Re-login failed for keyword {keyword}")
                        # Wait longer after re-login before retrying
                        time.sleep(random.uniform(5, 8))
                        # Retry navigation after re-login
                        self.navigate_to(search_url)
                        time.sleep(5)
                    
                    # Wait for search results to load
                    time.sleep(5)
                    
                    # Try to wait for specific elements that indicate content loaded
                    try:
                        # Wait for any post-like element to appear (with retry)
                        self._wait_for_selector_safe(
                            'div[role="article"], article, div[data-pagelet*="FeedUnit"], div[data-pagelet*="SearchResults"]',
                            timeout=10000,
                            state='visible'
                        )
                    except:
                        # If no posts found, try scrolling to trigger lazy loading
                        self.page.mouse.wheel(0, 500)
                        time.sleep(2)
                    
                    posts = self._extract_posts_from_page(posts_per_keyword)
                
                if posts:
                    self.logger.info(
                        "Posts extracted",
                        extra={'keyword': keyword, 'count': len(posts)}
                    )
                    
                    # Process hashtags from posts
                    for post in posts:
                        hashtags = self._extract_hashtags_from_post(post, category)
                        relevant_hashtags = [
                            tag for tag in hashtags
                            if self._is_relevant_hashtag(tag, category)
                        ]
                        
                        for tag in relevant_hashtags:
                            tag_lower = tag.lower()
                            
                            if tag_lower in all_hashtag_data:
                                # Update existing hashtag data
                                self._update_hashtag_data(
                                    all_hashtag_data[tag_lower],
                                    post
                                )
                            else:
                                # Create new hashtag entry
                                all_hashtag_data[tag_lower] = self._create_hashtag_entry(
                                    tag,
                                    category,
                                    post
                                )
                else:
                    self.logger.warning(
                        "No posts found",
                        extra={'keyword': keyword}
                    )
                
                # Longer delay between keywords to reduce rate limiting
                delay = random.uniform(5, 8)
                self.logger.debug(f"Waiting {delay:.1f}s before next keyword")
                time.sleep(delay)
                
            except Exception as e:
                self.logger.error(
                    "Keyword search failed",
                    extra={'keyword': keyword, 'error': str(e)}
                )
                # If error is fatal (RuntimeError from proxy/login), re-raise to crash
                if isinstance(e, RuntimeError) or isinstance(e, ValueError):
                    self.logger.critical("FATAL ERROR during keyword scrape. Crashing.")
                    raise e
                    
                # Wait even on error to avoid rapid retries
                time.sleep(random.uniform(3, 5))
                continue
        
        # Calculate final metrics and sort
        results = self._finalize_hashtag_data(all_hashtag_data, category)
        
        self.logger.info(
            "Category scrape complete",
            extra={'category': category, 'hashtags_found': len(results)}
        )
        
        # Update metrics if available (for IndustrialFacebookScraper)
        if hasattr(self, 'metrics'):
            self.metrics.total_hashtags_found += len(results)
        
        return results
    
    @retry_extraction(max_attempts=2)
    def _extract_posts_from_page(self, max_posts: int = 20) -> List[Dict]:
        """
        Extract posts from current page with scrolling.
        
        Args:
            max_posts: Maximum posts to extract
            
        Returns:
            List of post dictionaries
        """
        posts = []
        scrolls = 0
        # More aggressive scrolling: up to 40 scrolls or max_posts/1.5
        max_scrolls = min(40, int(max_posts * 1.5))  # More scrolls for more posts
        consecutive_empty = 0
        last_post_count = 0
        containers_found_total = 0
        posts_skipped_reasons = {'empty_text': 0, 'too_short': 0, 'duplicate': 0}
        
        # Wait for page to load initially
        time.sleep(2)  # Reduced initial wait
        
        while scrolls < max_scrolls and len(posts) < max_posts:
            try:
                # Check if page is still alive
                try:
                    self.page.url  # Quick check if page exists
                except:
                    self.logger.warning("Page closed during scraping, stopping")
                    break
                
                time.sleep(random.uniform(0.8, 1.2))  # Reduced wait times for speed
                
                # Try multiple selectors for post containers (expanded list)
                selectors = [
                    'div[role="article"]',
                    'div[data-pagelet*="FeedUnit"]',
                    'div[data-pagelet*="SearchResults"]',
                    'div[data-pagelet*="Stories"]',
                    'div[class*="userContentWrapper"]',
                    'div[class*="story_body_container"]',
                    'div[class*="post"]',
                    'article',
                    'div[data-ad-preview="message"]',
                    'div[data-testid*="post"]',
                ]
                containers = []
                
                for selector in selectors:
                    try:
                        found = self.page.locator(selector).all()
                        if found and len(found) > len(containers):
                            containers = found
                            if self.debug:
                                self.logger.debug(
                                    "Found containers with selector",
                                    extra={'selector': selector, 'count': len(found)}
                                )
                    except:
                        continue
                
                if not containers:
                    consecutive_empty += 1
                    if self.debug:
                        self.logger.debug(
                            "No containers found",
                            extra={'scroll': scrolls, 'consecutive_empty': consecutive_empty}
                        )
                    
                    # Debug: Save screenshot and inspect page on first failure
                    if consecutive_empty == 1 and self.debug:
                        try:
                            debug_dir = Path(__file__).parent / 'debug'
                            debug_dir.mkdir(exist_ok=True)
                            screenshot_path = debug_dir / f'debug_no_posts_{int(time.time())}.png'
                            self.page.screenshot(path=str(screenshot_path))
                            self.logger.debug(
                                "Debug screenshot saved",
                                extra={'path': str(screenshot_path)}
                            )
                            
                            # Try to get page title and URL for debugging
                            page_title = self.page.title()
                            page_url = self.page.url
                            self.logger.debug(
                                "Page info",
                                extra={'title': page_title, 'url': page_url}
                            )
                            
                            # Try to find any divs with text content
                            try:
                                all_divs = self.page.locator('div').count()
                                visible_divs = self.page.locator('div:visible').count()
                                self.logger.debug(
                                    "Page structure",
                                    extra={
                                        'total_divs': all_divs,
                                        'visible_divs': visible_divs,
                                        'body_text_length': len(self.page.locator('body').inner_text()[:500])
                                    }
                                )
                            except:
                                pass
                        except Exception as e:
                            self.logger.debug("Debug screenshot failed", extra={'error': str(e)})
                    
                    # Only break if we've had many consecutive empty results AND no new posts
                    if consecutive_empty >= 3 and len(posts) == last_post_count:
                        if self.debug:
                            self.logger.debug("Breaking due to consecutive empty results and no new posts")
                        break
                    # Scroll before trying again
                    try:
                        self.page.url  # Check page is alive
                        scroll_amount = random.randint(800, 1200)
                        self.page.mouse.wheel(0, scroll_amount)
                        time.sleep(random.uniform(1.0, 1.5))  # Reduced wait time
                        scrolls += 1
                    except Exception as e:
                        self.logger.warning(f"Page closed: {e}")
                        break
                    continue
                
                new_posts_found = 0
                last_post_count = len(posts)
                consecutive_empty = 0  # Reset on finding containers
                containers_found_total += len(containers)
                
                if self.debug and len(containers) > 0:
                    self.logger.debug(f"Processing {len(containers)} containers (total found: {containers_found_total}, posts extracted: {len(posts)})")
                
                for container in containers:
                    try:
                        # Try to get text with multiple methods - more aggressive approach
                        text = ""
                        
                        # Strategy 1: Try direct inner_text first (fastest)
                        try:
                            full_text = container.inner_text(timeout=2000).strip()
                            # Filter out very short or mostly whitespace
                            if len(full_text) > 10:
                                text = full_text
                        except:
                            pass
                        
                        # Strategy 2: If direct text failed, try extracting from specific elements
                        if not text or len(text) < 10:
                            text_selectors = [
                                'div[data-ad-preview="message"]',
                                'div[class*="userContent"]',
                                'div[class*="post_message"]',
                                '[data-testid="post_message"]',
                                'div[dir="auto"]',
                                'span[dir="auto"]',
                                'div[class*="x11i5rnm"]',  # Post text
                                'div[class*="x193iq5w"]',  # Text container
                                'span[class*="x193iq5w"]', # Text span
                                'p',
                                'div[class*="text"]',
                            ]
                            
                            for text_selector in text_selectors:
                                try:
                                    text_elem = container.locator(text_selector)
                                    if text_elem.count() > 0:
                                        # Try first element
                                        try:
                                            candidate_text = text_elem.first.inner_text(timeout=1500).strip()
                                            if len(candidate_text) > len(text):
                                                text = candidate_text
                                        except:
                                            pass
                                        
                                        # If we have multiple, try combining them
                                        if text_elem.count() > 1:
                                            try:
                                                all_texts = []
                                                for i in range(min(3, text_elem.count())):  # Try first 3
                                                    try:
                                                        elem_text = text_elem.nth(i).inner_text(timeout=1000).strip()
                                                        if elem_text and len(elem_text) > 5:
                                                            all_texts.append(elem_text)
                                                    except:
                                                        continue
                                                if all_texts:
                                                    combined = " ".join(all_texts)
                                                    if len(combined) > len(text):
                                                        text = combined
                                            except:
                                                pass
                                        
                                        if text and len(text) >= 10:
                                            break
                                except:
                                    continue
                        
                        # Strategy 3: Try getting all text nodes and combining
                        if not text or len(text) < 10:
                            try:
                                # Get all visible text elements
                                all_text_elements = container.locator('*:visible').all()
                                text_parts = []
                                for elem in all_text_elements[:10]:  # Limit to first 10 to avoid too much
                                    try:
                                        elem_text = elem.inner_text(timeout=500).strip()
                                        # Filter out very short or common UI text
                                        if elem_text and len(elem_text) > 15 and elem_text not in ['Like', 'Comment', 'Share', 'Follow', 'See more']:
                                            text_parts.append(elem_text)
                                    except:
                                        continue
                                
                                if text_parts:
                                    # Combine and deduplicate
                                    combined = " ".join(text_parts)
                                    # Remove duplicates (simple approach)
                                    words = combined.split()
                                    unique_words = []
                                    seen = set()
                                    for word in words:
                                        if word.lower() not in seen:
                                            unique_words.append(word)
                                            seen.add(word.lower())
                                    combined = " ".join(unique_words)
                                    
                                    if len(combined) > len(text):
                                        text = combined
                            except:
                                pass
                        
                        # Skip short posts or empty text (lowered threshold for more posts)
                        if not text:
                            posts_skipped_reasons['empty_text'] += 1
                            if self.debug:
                                self.logger.debug("Skipping post: empty text")
                            continue
                        
                        if len(text) < 10:  # Lowered from 20 to 10 for more posts
                            posts_skipped_reasons['too_short'] += 1
                            if self.debug:
                                self.logger.debug(f"Skipping post: text too short ({len(text)} chars)")
                            continue
                        
                        # Check for duplicates
                        text_hash = self.generate_text_hash(text)
                        if text_hash in self.seen_text_hashes:
                            posts_skipped_reasons['duplicate'] += 1
                            if self.debug:
                                self.logger.debug("Skipping post: duplicate")
                            continue
                        self.seen_text_hashes.add(text_hash)
                        
                        # Extract engagement metrics
                        likes, comments, shares, is_estimated = self._extract_engagement(
                            container,
                            text
                        )
                        engagement = likes + comments + shares
                        
                        if self.debug:
                            self.logger.debug(
                                "Post processed",
                                extra={
                                    'preview': text[:80],
                                    'likes': likes,
                                    'comments': comments,
                                    'shares': shares,
                                    'estimated': is_estimated
                                }
                            )
                        
                        # Analyze sentiment
                        sentiment, polarity, subjectivity = self.analyze_sentiment(text)
                        
                        # Detect language
                        language, lang_confidence = self.detect_language(text)
                        
                        # Extract post type and media
                        post_type, media_info = self._extract_post_type_and_media(container)
                        
                        posts.append({
                            'text': text,
                            'likes': likes,
                            'comments': comments,
                            'shares': shares,
                            'engagement': engagement,
                            'sentiment': sentiment,
                            'sentiment_score': polarity,
                            'subjectivity': subjectivity,
                            'is_estimated': is_estimated,
                            'post_type': post_type,
                            'language': language,
                            'language_confidence': lang_confidence,
                            **media_info  # Unpack media info (images, videos, links, etc.)
                        })
                        
                        new_posts_found += 1
                        
                        if len(posts) >= max_posts:
                            break
                        
                    except Exception as e:
                        if self.debug:
                            self.logger.debug(
                                "Container processing error",
                                extra={'error': str(e)}
                            )
                        continue
                
                # Track if we found new posts
                if new_posts_found == 0:
                    consecutive_empty += 1
                    # Break if we've had multiple empty scrolls (Facebook showing same content)
                    # Increased to 4 to be more persistent
                    if consecutive_empty >= 4:
                        if self.debug:
                            self.logger.debug(f"Breaking: no new posts after {consecutive_empty} scrolls (found {len(posts)} total)")
                        break
                else:
                    consecutive_empty = 0  # Reset when we find posts
                
                # Continue scrolling if we haven't reached max_posts
                if len(posts) < max_posts:
                    try:
                        # Check page is still alive before scrolling
                        self.page.url
                        # More aggressive scrolling with larger distances
                        scroll_distance = random.randint(1000, 1500)
                        self.page.mouse.wheel(0, scroll_distance)
                        # Wait a bit longer for lazy loading
                        time.sleep(random.uniform(1.2, 1.8))
                        scrolls += 1
                    except Exception as e:
                        self.logger.warning(f"Page closed during scroll: {e}")
                        break
                else:
                    if self.debug:
                        self.logger.debug(f"Reached max_posts ({max_posts}), stopping scroll")
                    break  # We have enough posts
                
            except Exception as e:
                if self.debug:
                    self.logger.debug("Scroll error", extra={'error': str(e)})
                scrolls += 1
                continue
        
        # Log extraction statistics
        extraction_rate = (len(posts) / containers_found_total * 100) if containers_found_total > 0 else 0
        self.logger.info(
            "Posts extracted from page",
            extra={
                'count': len(posts),
                'scrolls': scrolls,
                'containers_found': containers_found_total,
                'extraction_rate': f"{extraction_rate:.1f}%",
                'skipped': posts_skipped_reasons
            }
        )
        
        # Update metrics if available (for IndustrialFacebookScraper)
        if hasattr(self, 'metrics'):
            self.metrics.total_posts_scraped += len(posts)
        
        return posts
    
    def _extract_engagement(self, container, text: str) -> Tuple[int, int, int, bool]:
        """
        Extract engagement metrics from post container.
        
        Args:
            container: Playwright element locator
            text: Post text content
            
        Returns:
            tuple: (likes, comments, shares, is_estimated)
        """
        try:
            likes = comments = shares = 0
            is_estimated = False
            
            # Get container HTML and text
            try:
                container_html = container.inner_html(timeout=1000)
                container_text = container.inner_text(timeout=1000)
            except:
                container_html = ""
                container_text = text
            
            # Comprehensive engagement patterns
            patterns = {
                'likes': [
                    r'aria-label="(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+(?:reaction|like)',
                    r'(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+(?:like|reaction)s?',
                    r'>(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s*<.*?(?:like|reaction)',
                ],
                'comments': [
                    r'aria-label="(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+comment',
                    r'(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+comments?',
                    r'>(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s*<.*?comment',
                ],
                'shares': [
                    r'aria-label="(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+share',
                    r'(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s+shares?',
                    r'>(\d+(?:,\d{3})*(?:\.\d+)?[KMB]?)\s*<.*?share',
                ]
            }
            
            search_content = f"{container_html} {container_text}"
            
            # Extract metrics using patterns
            for metric, pattern_list in patterns.items():
                for pattern in pattern_list:
                    matches = re.findall(pattern, search_content, re.I)
                    if matches:
                        numbers = [self._parse_number(m) for m in matches]
                        number = max(numbers) if numbers else 0
                        
                        if number > 0:
                            if metric == 'likes':
                                likes = max(likes, number)
                            elif metric == 'comments':
                                comments = max(comments, number)
                            elif metric == 'shares':
                                shares = max(shares, number)
                            break
            
            # Estimate missing metrics
            if likes > 0 or comments > 0 or shares > 0:
                if likes == 0 and comments > 0:
                    likes = int(comments * random.uniform(8.0, 12.0))
                    is_estimated = True
                elif likes == 0 and shares > 0:
                    likes = int(shares * random.uniform(15.0, 20.0))
                    is_estimated = True
                
                if comments == 0 and likes > 0:
                    comments = int(likes * random.uniform(0.08, 0.12))
                    is_estimated = True
                
                if shares == 0 and likes > 0:
                    shares = int(likes * random.uniform(0.03, 0.06))
                    is_estimated = True
            else:
                # Conservative baseline
                content_quality = min(len(text) // 50, 10)
                base = random.randint(100 * content_quality, 300 * content_quality)
                
                likes = base
                comments = int(base * 0.10)
                shares = int(base * 0.04)
                is_estimated = True
            
            return likes, comments, shares, is_estimated
            
        except Exception as e:
            if self.debug:
                self.logger.debug(
                    "Engagement extraction error",
                    extra={'error': str(e)}
                )
            base = random.randint(150, 500)
            return base, int(base * 0.10), int(base * 0.04), True
    
    def _parse_number(self, text: str) -> int:
        """
        Parse number with K/M/B multipliers.
        
        Args:
            text: Number string (e.g., "1.5K", "2M")
            
        Returns:
            int: Parsed number
        """
        try:
            text = text.replace(',', '').strip().upper()
            multiplier = 1
            
            if 'K' in text:
                multiplier = 1000
                text = text.replace('K', '')
            elif 'M' in text:
                multiplier = 1000000
                text = text.replace('M', '')
            elif 'B' in text:
                multiplier = 1000000000
                text = text.replace('B', '')
            
            return int(float(text) * multiplier)
        except:
            return 0
    
    def _extract_post_type_and_media(self, container) -> Tuple[str, Dict]:
        """
        Extract post type and media information from post container.
        
        Args:
            container: Playwright element locator for post container
            
        Returns:
            Tuple of (post_type, media_info_dict)
            - post_type: "text", "image", "video", "link", "album", "mixed", or "unknown"
            - media_info_dict: Dictionary with media URLs and formats
        """
        post_type = "text"  # Default to text post
        media_info = {
            'images': [],
            'videos': [],
            'links': [],
            'has_video': False,
            'has_images': False,
            'has_link': False,
            'media_count': 0
        }
        
        try:
            # Get container HTML for pattern matching
            try:
                container_html = container.inner_html(timeout=2000)
            except:
                container_html = ""
            
            # Check for images
            image_selectors = [
                'img[src*="scontent"]',
                'img[src*="fbcdn"]',
                'img[src*="facebook.com"]',
                'img[data-src*="scontent"]',
                'img[data-src*="fbcdn"]',
                'div[style*="background-image"]',
                'img[role="img"]',
                'a[href*="/photo"] img',
                'a[href*="/photos"] img'
            ]
            
            images_found = set()
            for selector in image_selectors:
                try:
                    img_elements = container.locator(selector).all()
                    for img in img_elements[:10]:  # Limit to first 10 images
                        try:
                            # Try src attribute first
                            src = img.get_attribute('src') or img.get_attribute('data-src')
                            if src and 'scontent' in src or 'fbcdn' in src:
                                # Filter out profile pictures and icons
                                if 'profile' not in src.lower() and 'icon' not in src.lower():
                                    if src not in images_found:
                                        images_found.add(src)
                                        media_info['images'].append(src)
                        except:
                            continue
                except:
                    continue
            
            # Check for videos
            video_selectors = [
                'video[src]',
                'video source[src]',
                'div[data-video-id]',
                'div[aria-label*="video"]',
                'a[href*="/video"]',
                'div[class*="video"]',
                'span[aria-label*="video"]'
            ]
            
            videos_found = set()
            for selector in video_selectors:
                try:
                    video_elements = container.locator(selector).all()
                    for video in video_elements[:5]:  # Limit to first 5 videos
                        try:
                            # Try video src
                            src = video.get_attribute('src')
                            if src:
                                if src not in videos_found:
                                    videos_found.add(src)
                                    media_info['videos'].append(src)
                            else:
                                # Try data-video-id or href
                                video_id = video.get_attribute('data-video-id') or \
                                          video.get_attribute('href') or \
                                          video.get_attribute('data-href')
                                if video_id and video_id not in videos_found:
                                    videos_found.add(video_id)
                                    media_info['videos'].append(video_id)
                        except:
                            continue
                except:
                    continue
            
            # Check for external links
            link_selectors = [
                'a[href*="http"][href*="facebook.com"]',
                'a[data-href]',
                'a[target="_blank"]'
            ]
            
            links_found = set()
            for selector in link_selectors:
                try:
                    link_elements = container.locator(selector).all()
                    for link in link_elements[:5]:  # Limit to first 5 links
                        try:
                            href = link.get_attribute('href') or link.get_attribute('data-href')
                            if href and href.startswith('http'):
                                # Filter out internal Facebook links
                                if 'facebook.com' not in href or any(x in href for x in ['/video', '/photo', '/photos']):
                                    # Keep external links or media-specific links
                                    if href not in links_found:
                                        links_found.add(href)
                                        media_info['links'].append(href)
                        except:
                            continue
                except:
                    continue
            
            # Also check HTML patterns for media
            if container_html:
                # Pattern matching for images in HTML
                import re
                img_patterns = [
                    r'url\(["\']?([^"\']*scontent[^"\']*)["\']?\)',
                    r'src=["\']([^"\']*scontent[^"\']*)["\']',
                    r'data-src=["\']([^"\']*scontent[^"\']*)["\']',
                    r'background-image:\s*url\(["\']?([^"\']*scontent[^"\']*)["\']?\)'
                ]
                
                for pattern in img_patterns:
                    matches = re.findall(pattern, container_html, re.IGNORECASE)
                    for match in matches:
                        if match and match not in images_found:
                            if 'profile' not in match.lower() and 'icon' not in match.lower():
                                images_found.add(match)
                                media_info['images'].append(match)
                
                # Pattern matching for videos
                video_patterns = [
                    r'data-video-id=["\']([^"\']+)["\']',
                    r'href=["\']([^"\']*/video/[^"\']+)["\']',
                    r'video[_-]id=["\']([^"\']+)["\']'
                ]
                
                for pattern in video_patterns:
                    matches = re.findall(pattern, container_html, re.IGNORECASE)
                    for match in matches:
                        if match and match not in videos_found:
                            videos_found.add(match)
                            media_info['videos'].append(match)
            
            # Determine post type based on media found
            has_images = len(media_info['images']) > 0
            has_videos = len(media_info['videos']) > 0
            has_links = len(media_info['links']) > 0
            
            media_info['has_images'] = has_images
            media_info['has_videos'] = has_videos
            media_info['has_link'] = has_links
            media_info['media_count'] = len(media_info['images']) + len(media_info['videos']) + len(media_info['links'])
            
            if has_videos and has_images:
                post_type = "mixed"
            elif has_videos:
                post_type = "video"
            elif has_images:
                if len(media_info['images']) > 1:
                    post_type = "album"
                else:
                    post_type = "image"
            elif has_links:
                post_type = "link"
            else:
                post_type = "text"
            
            # Limit media arrays to reasonable sizes
            media_info['images'] = media_info['images'][:10]
            media_info['videos'] = media_info['videos'][:5]
            media_info['links'] = media_info['links'][:5]
            
        except Exception as e:
            self.logger.debug(f"Error extracting post type/media: {e}")
            # Return defaults if extraction fails
            pass
        
        return post_type, media_info
    
    def _extract_hashtags_from_post(self, post: Dict, category: str) -> List[str]:
        """
        Extract hashtags from post text.
        
        Args:
            post: Post dictionary with 'text' key
            category: Category name for fallback
            
        Returns:
            List of hashtag strings
        """
        text = post['text']
        
        # Extract explicit hashtags
        explicit_tags = re.findall(r'#(\w+)', text)
        
        # Fallback to category defaults if no hashtags found
        if not explicit_tags:
            cat_data = self.categories.get(category.lower(), {})
            explicit_tags = cat_data.get('hashtags', [category])[:5]
        
        # Extract keywords from text
        keywords = self._extract_keywords(text)
        
        # Combine and deduplicate
        all_tags = list(set(explicit_tags + keywords))
        
        return all_tags[:10]
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        Extract keywords from text using word frequency.
        
        Args:
            text: Post text
            
        Returns:
            List of keyword strings
        """
        # Common words to filter out
        common_words = {
            'this', 'that', 'with', 'from', 'have', 'more', 'will', 'their',
            'there', 'what', 'about', 'which', 'when', 'make', 'like', 'time',
            'just', 'know', 'take', 'people', 'into', 'year', 'your', 'some',
            'could', 'them', 'than', 'other', 'then', 'look', 'only', 'come',
            'over', 'think', 'also', 'back', 'after', 'work', 'first', 'well',
            'even', 'want', 'because', 'these', 'give', 'most'
        }
        
        # Extract words
        words = re.findall(r'\b[a-z]{4,}\b', text.lower())
        filtered = [w for w in words if w not in common_words]
        
        # Count word frequency
        word_counts = Counter(filtered)
        
        # Return most common words that appear at least twice
        return [word for word, count in word_counts.most_common(5) if count >= 2]
    
    def _is_relevant_hashtag(self, hashtag: str, category: str) -> bool:
        """
        Check if hashtag is relevant to category.
        
        Args:
            hashtag: Hashtag string
            category: Category name
            
        Returns:
            bool: True if relevant
        """
        cat_data = self.categories.get(category.lower(), {})
        keywords = cat_data.get('keywords', [])
        predefined = cat_data.get('hashtags', [])
        
        hashtag_lower = hashtag.lower()
        
        # Direct match
        if hashtag_lower in [k.lower() for k in keywords + predefined]:
            return True
        
        # Partial match
        for keyword in keywords:
            if keyword.lower() in hashtag_lower or hashtag_lower in keyword.lower():
                return True
        
        # Reject if too short
        if len(hashtag_lower) < 3:
            return False
        
        return True
    
    def _create_hashtag_entry(self, tag: str, category: str, post: Dict) -> Dict:
        """
        Create initial hashtag data entry.
        
        Args:
            tag: Hashtag string
            category: Category name
            post: Post dictionary
            
        Returns:
            Dict: Hashtag data entry
        """
        post_type = post.get('post_type', 'text')
        media_info = {
            'images': post.get('images', []),
            'videos': post.get('videos', []),
            'links': post.get('links', []),
            'has_images': post.get('has_images', False),
            'has_videos': post.get('has_videos', False),
            'has_link': post.get('has_link', False)
        }
        
        language = post.get('language', 'en')
        lang_confidence = post.get('language_confidence', 0.0)
        
        return {
            'hashtag': tag,
            'category': category,
            'post_count': 1,
            'total_engagement': post['engagement'],
            'likes': post['likes'],
            'comments': post['comments'],
            'shares': post['shares'],
            'sentiment': post['sentiment'],
            'sentiment_score': post['sentiment_score'],
            'engagement_list': [post['engagement']],
            'timestamp': datetime.now(),
            'is_estimated': post.get('is_estimated', False),
            # Post type and media statistics
            'post_types': {post_type: 1},
            'media_stats': {
                'total_images': len(media_info['images']),
                'total_videos': len(media_info['videos']),
                'total_links': len(media_info['links']),
                'posts_with_images': 1 if media_info['has_images'] else 0,
                'posts_with_videos': 1 if media_info['has_videos'] else 0,
                'posts_with_links': 1 if media_info['has_link'] else 0
            },
            'media_samples': {
                'images': media_info['images'][:3],  # Store first 3 image URLs as samples
                'videos': media_info['videos'][:2],  # Store first 2 video URLs as samples
                'links': media_info['links'][:2]     # Store first 2 link URLs as samples
            },
            # Language statistics
            'languages': {language: 1},  # Track language distribution
            'language_confidences': [lang_confidence],  # Track confidence scores
            'primary_language': language  # Most common language (will be updated in _finalize)
        }
    
    def _update_hashtag_data(self, hashtag_data: Dict, post: Dict):
        """
        Update existing hashtag data with new post.
        
        Args:
            hashtag_data: Existing hashtag data dictionary
            post: New post dictionary
        """
        hashtag_data['post_count'] += 1
        hashtag_data['total_engagement'] += post['engagement']
        hashtag_data['likes'] += post['likes']
        hashtag_data['comments'] += post['comments']
        hashtag_data['shares'] += post['shares']
        hashtag_data['engagement_list'].append(post['engagement'])
        
        # Update estimated flag (if any post has real data, mark as not estimated)
        if not post.get('is_estimated', False):
            hashtag_data['is_estimated'] = False
        
        # Update language statistics
        language = post.get('language', 'en')
        lang_confidence = post.get('language_confidence', 0.0)
        
        if 'languages' not in hashtag_data:
            hashtag_data['languages'] = {}
        hashtag_data['languages'][language] = hashtag_data['languages'].get(language, 0) + 1
        
        if 'language_confidences' not in hashtag_data:
            hashtag_data['language_confidences'] = []
        hashtag_data['language_confidences'].append(lang_confidence)
        
        # Update post type statistics
        post_type = post.get('post_type', 'text')
        if 'post_types' not in hashtag_data:
            hashtag_data['post_types'] = {}
        hashtag_data['post_types'][post_type] = hashtag_data['post_types'].get(post_type, 0) + 1
        
        # Update media statistics
        media_info = {
            'images': post.get('images', []),
            'videos': post.get('videos', []),
            'links': post.get('links', []),
            'has_images': post.get('has_images', False),
            'has_videos': post.get('has_videos', False),
            'has_link': post.get('has_link', False)
        }
        
        if 'media_stats' not in hashtag_data:
            hashtag_data['media_stats'] = {
                'total_images': 0,
                'total_videos': 0,
                'total_links': 0,
                'posts_with_images': 0,
                'posts_with_videos': 0,
                'posts_with_links': 0
            }
        
        hashtag_data['media_stats']['total_images'] += len(media_info['images'])
        hashtag_data['media_stats']['total_videos'] += len(media_info['videos'])
        hashtag_data['media_stats']['total_links'] += len(media_info['links'])
        if media_info['has_images']:
            hashtag_data['media_stats']['posts_with_images'] += 1
        if media_info['has_videos']:
            hashtag_data['media_stats']['posts_with_videos'] += 1
        if media_info['has_link']:
            hashtag_data['media_stats']['posts_with_links'] += 1
        
        # Update media samples (keep most recent samples)
        if 'media_samples' not in hashtag_data:
            hashtag_data['media_samples'] = {'images': [], 'videos': [], 'links': []}
        
        # Add new samples if we have space
        if len(hashtag_data['media_samples']['images']) < 10:
            hashtag_data['media_samples']['images'].extend(media_info['images'][:3])
        if len(hashtag_data['media_samples']['videos']) < 5:
            hashtag_data['media_samples']['videos'].extend(media_info['videos'][:2])
        if len(hashtag_data['media_samples']['links']) < 5:
            hashtag_data['media_samples']['links'].extend(media_info['links'][:2])
    
    def _finalize_hashtag_data(self, hashtag_data: Dict, category: str) -> List[Dict]:
        """
        Calculate final metrics and sort hashtags.
        
        Args:
            hashtag_data: Dictionary of hashtag data
            category: Category name
            
        Returns:
            List of sorted hashtag dictionaries
        """
        results = []
        
        for tag_data in hashtag_data.values():
            count = tag_data['post_count']
            
            # Calculate averages
            tag_data['avg_engagement'] = tag_data['total_engagement'] / count
            tag_data['avg_likes'] = tag_data['likes'] / count
            tag_data['avg_comments'] = tag_data['comments'] / count
            tag_data['avg_shares'] = tag_data['shares'] / count
            
            # Calculate engagement score
            tag_data['engagement_score'] = self._calculate_engagement_score(
                int(tag_data['avg_likes']),
                int(tag_data['avg_comments']),
                int(tag_data['avg_shares'])
            )
            
            # Calculate trending score
            tag_data['trending_score'] = self._calculate_trending_score(tag_data)
            
            # Calculate virality score
            tag_data['virality_score'] = self._calculate_virality_score(tag_data)
            
            # Add hashtag URL
            tag_data['hashtag_url'] = f"https://www.facebook.com/hashtag/{tag_data['hashtag']}"
            
            # Determine primary language (most common language)
            languages = tag_data.get('languages', {})
            if languages:
                primary_language = max(languages.items(), key=lambda x: x[1])[0]
                tag_data['primary_language'] = primary_language
                tag_data['language_distribution'] = languages
            else:
                tag_data['primary_language'] = 'en'
                tag_data['language_distribution'] = {'en': count}
            
            # Calculate average language confidence
            lang_confidences = tag_data.get('language_confidences', [])
            if lang_confidences:
                tag_data['avg_language_confidence'] = round(sum(lang_confidences) / len(lang_confidences), 3)
            else:
                tag_data['avg_language_confidence'] = 0.0
            
            # Remove temporary fields
            tag_data.pop('engagement_list', None)
            tag_data.pop('timestamp', None)
            tag_data.pop('language_confidences', None)  # Remove raw list, keep avg
            
            results.append(tag_data)
        
        # Sort by trending score, virality score, engagement score, and post count
        results.sort(
            key=lambda x: (
                x['trending_score'], 
                x.get('virality_score', 0), 
                x['engagement_score'], 
                x['post_count']
            ),
            reverse=True
        )
        
        return results
    
    def _calculate_engagement_score(self, likes: int, comments: int, shares: int) -> float:
        """
        Calculate engagement score (1-10 scale).
        
        Args:
            likes: Number of likes
            comments: Number of comments
            shares: Number of shares
            
        Returns:
            float: Engagement score (1.0-10.0)
        """
        try:
            # Weighted engagement calculation
            weighted = (likes * 1) + (comments * 4) + (shares * 8)
            
            if weighted == 0:
                return 1.0
            
            # Progressive scaling
            if weighted <= 20:
                score = 1.0 + (weighted / 20) * 1.5
            elif weighted <= 100:
                score = 2.5 + ((weighted - 20) / 80) * 1.5
            elif weighted <= 500:
                score = 4.0 + ((weighted - 100) / 400) * 2.0
            elif weighted <= 2000:
                score = 6.0 + ((weighted - 500) / 1500) * 2.0
            elif weighted <= 10000:
                score = 8.0 + ((weighted - 2000) / 8000) * 1.5
            else:
                score = min(10.0, 9.5 + (math.log10(weighted) - 4) * 0.125)
            return round(max(1.0, min(10.0, score)), 2)
        except:
            return 1.0
    
    def _calculate_trending_score(self, hashtag_data: Dict, time_weight: float = 0.20) -> float:
        """
        Calculate comprehensive trending score with enhanced analytics (0-100 scale).
        
        Features:
        - Time-weighted trend scoring with exponential decay
        - Sentiment weighting with polarity amplification
        - Engagement normalization using logarithmic scaling
        - Velocity calculation (engagement growth rate)
        - Consistency scoring with coefficient of variation
        
        Args:
            hashtag_data: Hashtag data dictionary
            time_weight: Weight for time decay factor (default: 0.20)
            
        Returns:
            float: Trending score (0-100)
        """
        engagement = hashtag_data.get('engagement_score', 0)
        post_count = hashtag_data.get('post_count', 0)
        total_engagement = hashtag_data.get('total_engagement', 0)
        avg_engagement = hashtag_data.get('avg_engagement', 0)
        sentiment_score = hashtag_data.get('sentiment_score', 0)
        
        # === Enhanced Normalization with Logarithmic Scaling ===
        eng_norm = min(engagement / 10.0, 1.0)
        post_norm = min(math.log1p(post_count) / math.log1p(25), 1.0)
        total_norm = min(math.log1p(total_engagement) / math.log1p(25000), 1.0)
        avg_norm = min(math.log1p(avg_engagement) / math.log1p(2500), 1.0)
        
        # === Sentiment Weighting with Polarity Amplification ===
        if sentiment_score > 0:
            sentiment_weight = 1.0 + (sentiment_score * 0.3)
        elif sentiment_score < 0:
            sentiment_weight = 1.0 + (sentiment_score * 0.2)
        else:
            sentiment_weight = 1.0
        
        sentiment_norm = (sentiment_score + 1) / 2
        
        # === Time-Weighted Decay ===
        time_factor = 1.0
        if 'timestamp' in hashtag_data:
            hours_ago = (datetime.now() - hashtag_data['timestamp']).total_seconds() / 3600
            time_factor = math.exp(-hours_ago / 12.0)
        
        # === Enhanced Virality Metrics ===
        virality_score = self._calculate_virality_score(hashtag_data)
        
        # === Engagement Consistency & Velocity ===
        consistency = 1.0
        velocity = 0.0
        acceleration = 0.0
        viral_coefficient = 0.0
        
        if 'engagement_list' in hashtag_data and len(hashtag_data['engagement_list']) > 1:
            engagements = hashtag_data['engagement_list']
            mean = sum(engagements) / len(engagements)
            
            if mean > 0:
                variance = sum((x - mean) ** 2 for x in engagements) / len(engagements)
                std_dev = math.sqrt(variance)
                cv = std_dev / mean
                consistency = 1.0 / (1.0 + cv)
            
            # Enhanced velocity calculation with exponential growth detection
            if len(engagements) >= 3:
                # Split into thirds for better growth detection
                third = len(engagements) // 3
                if third > 0:
                    early_third = engagements[:third]
                    mid_third = engagements[third:2*third] if len(engagements) >= 2*third else engagements[third:]
                    recent_third = engagements[2*third:] if len(engagements) >= 3*third else engagements[len(engagements)//2:]
                    
                    early_avg = sum(early_third) / len(early_third) if early_third else 0
                    mid_avg = sum(mid_third) / len(mid_third) if mid_third else 0
                    recent_avg = sum(recent_third) / len(recent_third) if recent_third else 0
                    
                    # Calculate velocity (growth rate)
                    if early_avg > 0:
                        velocity = min((recent_avg - early_avg) / early_avg, 2.0)  # Allow up to 200% growth
                        velocity = max(velocity, -0.5)
                    
                    # Calculate acceleration (change in growth rate)
                    if early_avg > 0 and mid_avg > 0:
                        early_to_mid = (mid_avg - early_avg) / early_avg if early_avg > 0 else 0
                        mid_to_recent = (recent_avg - mid_avg) / mid_avg if mid_avg > 0 else 0
                        acceleration = mid_to_recent - early_to_mid
                        acceleration = min(max(acceleration, -1.0), 1.0)
                    
                    # Detect exponential growth pattern
                    if len(engagements) >= 4:
                        # Check if growth is accelerating (exponential)
                        growth_rates = []
                        for i in range(1, len(engagements)):
                            if engagements[i-1] > 0:
                                rate = (engagements[i] - engagements[i-1]) / engagements[i-1]
                                growth_rates.append(rate)
                        
                        if growth_rates and len(growth_rates) >= 2:
                            # Check if growth rates are increasing (exponential pattern)
                            avg_early_growth = sum(growth_rates[:len(growth_rates)//2]) / (len(growth_rates)//2)
                            avg_recent_growth = sum(growth_rates[len(growth_rates)//2:]) / (len(growth_rates) - len(growth_rates)//2)
                            if avg_early_growth > 0 and avg_recent_growth > avg_early_growth * 1.2:
                                # Exponential growth detected - boost velocity
                                velocity = min(velocity * 1.5, 2.0)
        
        # Calculate viral coefficient (share-to-engagement ratio)
        total_shares = hashtag_data.get('shares', 0)
        if total_engagement > 0:
            viral_coefficient = min(total_shares / total_engagement, 1.0)  # Shares per total engagement
        
        # === Weighted Score Calculation with Enhanced Virality ===
        base_score = (
            eng_norm * 0.18 +                    # Engagement (18%, reduced from 22%)
            post_norm * 0.15 +                   # Post count (15%, reduced from 18%)
            total_norm * 0.10 +                  # Total engagement (10%, reduced from 12%)
            avg_norm * 0.10 +                    # Average engagement (10%, reduced from 12%)
            sentiment_norm * 0.08 +              # Sentiment (8%, same)
            time_factor * time_weight +         # Time decay (20%, same)
            consistency * 0.04 +                 # Consistency (4%, same)
            max(velocity, 0) * 0.10 +            # Velocity (10%, increased from 4%)
            max(acceleration, 0) * 0.05 +        # Acceleration (5%, new)
            viral_coefficient * 0.10              # Viral coefficient (10%, new)
        )
        
        # Apply sentiment weight multiplier
        base_score *= sentiment_weight
        
        # Scale to 0-100
        final_score = base_score * 100
        
        # Length bonus (prefer concise hashtags)
        length_factor = min(len(hashtag_data.get('hashtag', '')), 20) * 0.01
        final_score += length_factor
        
        # Apply virality boost
        final_score = base_score * 100
        if virality_score > 0.7:  # High virality
            final_score *= (1.0 + virality_score * 0.2)  # Up to 20% boost
        
        # Length bonus (prefer concise hashtags)
        length_factor = min(len(hashtag_data.get('hashtag', '')), 20) * 0.01
        final_score += length_factor
        
        return round(min(max(final_score, 0), 100), 2)
    
    def _calculate_virality_score(self, hashtag_data: Dict) -> float:
        """
        Calculate dedicated virality score (0-1 scale) based on growth patterns.
        
        Virality indicators:
        - Exponential growth rate
        - Share-to-engagement ratio (viral coefficient)
        - Acceleration in engagement
        - High share percentage
        - Rapid engagement growth
        
        Args:
            hashtag_data: Hashtag data dictionary
            
        Returns:
            float: Virality score (0.0-1.0)
        """
        try:
            virality_factors = []
            
            # Factor 1: Share-to-engagement ratio (viral coefficient)
            total_shares = hashtag_data.get('shares', 0)
            total_engagement = hashtag_data.get('total_engagement', 0)
            if total_engagement > 0:
                share_ratio = total_shares / total_engagement
                # High share ratio indicates virality (shares are more valuable)
                virality_factors.append(min(share_ratio * 5, 1.0))  # Scale up share importance
            
            # Factor 2: Engagement growth velocity
            if 'engagement_list' in hashtag_data and len(hashtag_data['engagement_list']) >= 3:
                engagements = hashtag_data['engagement_list']
                # Calculate growth rate
                if len(engagements) >= 2:
                    early_avg = sum(engagements[:len(engagements)//2]) / (len(engagements)//2)
                    recent_avg = sum(engagements[len(engagements)//2:]) / (len(engagements) - len(engagements)//2)
                    
                    if early_avg > 0:
                        growth_rate = (recent_avg - early_avg) / early_avg
                        # Normalize growth rate (0-1 scale, with 100%+ growth = 1.0)
                        virality_factors.append(min(max(growth_rate, 0) / 1.0, 1.0))
            
            # Factor 3: Average shares per post (viral spread indicator)
            post_count = hashtag_data.get('post_count', 1)
            avg_shares = total_shares / post_count if post_count > 0 else 0
            avg_engagement = hashtag_data.get('avg_engagement', 0)
            if avg_engagement > 0:
                share_percentage = avg_shares / avg_engagement
                # High share percentage = more viral
                virality_factors.append(min(share_percentage * 10, 1.0))
            
            # Factor 4: Engagement consistency with upward trend
            if 'engagement_list' in hashtag_data and len(hashtag_data['engagement_list']) >= 4:
                engagements = hashtag_data['engagement_list']
                # Check if trend is consistently upward
                upward_count = 0
                for i in range(1, len(engagements)):
                    if engagements[i] > engagements[i-1]:
                        upward_count += 1
                upward_ratio = upward_count / (len(engagements) - 1)
                virality_factors.append(upward_ratio)
            
            # Factor 5: High absolute engagement with growth
            if total_engagement > 1000 and 'engagement_list' in hashtag_data:
                # High engagement + growth = viral
                virality_factors.append(min(total_engagement / 10000, 1.0))
            
            # Calculate weighted average
            if virality_factors:
                # Weight later factors more (they're more indicative of virality)
                weights = [0.15, 0.25, 0.25, 0.20, 0.15]
                weighted_sum = sum(factor * weights[i] for i, factor in enumerate(virality_factors[:len(weights)]))
                weight_sum = sum(weights[:len(virality_factors)])
                return min(weighted_sum / weight_sum if weight_sum > 0 else 0, 1.0)
            
            return 0.0
            
        except Exception as e:
            self.logger.debug(f"Error calculating virality score: {e}")
            return 0.0
    
    def get_top_10_trending(self, category: str, max_posts: int = 100, 
                           sentiment_filter: Optional[str] = None,
                           min_sentiment: Optional[float] = None,
                           brand_safe_only: bool = False) -> List[Dict]:
        """
        Get top 10 trending hashtags for a category.
        
        Args:
            category: Category name
            max_posts: Maximum posts to process
            sentiment_filter: Filter by sentiment ("positive", "negative", "neutral", or None)
            min_sentiment: Minimum sentiment score threshold (default: None = no filter)
            brand_safe_only: If True, only return brand-safe trends (non-negative)
            
        Returns:
            List of top 10 hashtag dictionaries
        """
        all_results = self.scrape_category_hashtags(category, max_posts)
        
        if not all_results:
            self.logger.warning(
                "No hashtags found, using fallback",
                extra={'category': category}
            )
            return self._generate_fallback_top10(category)
        
        # Apply sentiment filtering if requested
        if sentiment_filter or min_sentiment is not None or brand_safe_only:
            if ENHANCED_SENTIMENT_AVAILABLE:
                try:
                    from sentiment_analyzer import get_analyzer, SentimentMethod
                    analyzer = get_analyzer(SentimentMethod.AUTO)
                    
                    # Apply brand safety filter
                    if brand_safe_only:
                        min_sentiment = max(min_sentiment or 0.0, 0.0)
                    
                    # Filter by sentiment
                    all_results = analyzer.filter_by_sentiment(
                        items=all_results,
                        sentiment_filter=sentiment_filter,
                        min_polarity=min_sentiment
                    )
                    self.logger.info(
                        "Applied sentiment filtering",
                        extra={
                            'sentiment_filter': sentiment_filter,
                            'min_sentiment': min_sentiment,
                            'brand_safe_only': brand_safe_only,
                            'filtered_count': len(all_results)
                        }
                    )
                except Exception as e:
                    self.logger.debug(f"Enhanced sentiment filtering failed: {e}, using simple filter")
                    # Fallback to simple filtering
                    if brand_safe_only or min_sentiment is not None:
                        filtered = []
                        min_threshold = min_sentiment if min_sentiment is not None else (0.0 if brand_safe_only else None)
                        for item in all_results:
                            sentiment = item.get('sentiment', 'neutral')
                            score = item.get('sentiment_score', 0.0)
                            
                            # Apply filters
                            if sentiment_filter and sentiment != sentiment_filter:
                                continue
                            if min_threshold is not None and score < min_threshold:
                                continue
                            if brand_safe_only and sentiment == "negative":
                                continue
                            filtered.append(item)
                        all_results = filtered
            else:
                # Simple filtering without enhanced analyzer
                if brand_safe_only or min_sentiment is not None or sentiment_filter:
                    filtered = []
                    min_threshold = min_sentiment if min_sentiment is not None else (0.0 if brand_safe_only else None)
                    for item in all_results:
                        sentiment = item.get('sentiment', 'neutral')
                        score = item.get('sentiment_score', 0.0)
                        
                        # Apply filters
                        if sentiment_filter and sentiment != sentiment_filter:
                            continue
                        if min_threshold is not None and score < min_threshold:
                            continue
                        if brand_safe_only and sentiment == "negative":
                            continue
                        filtered.append(item)
                    all_results = filtered
        
        top_10 = all_results[:10]
        
        self.logger.info(
            "Top 10 hashtags selected",
            extra={
                'category': category, 
                'count': len(top_10),
                'sentiment_filter': sentiment_filter,
                'brand_safe_only': brand_safe_only
            }
        )
        
        return top_10
    
    def _generate_fallback_top10(self, category: str) -> List[Dict]:
        """
        Generate fallback top 10 hashtags from predefined list.
        
        Args:
            category: Category name
            
        Returns:
            List of fallback hashtag dictionaries
        """
        cat_data = self.categories.get(category.lower(), {})
        fallback_tags = cat_data.get('hashtags', [category])[:10]
        
        results = []
        for i, tag in enumerate(fallback_tags):
            base = random.randint(2000, 8000) - (i * 300)
            l = int(base * 0.65)
            c = int(base * 0.25)
            s = int(base * 0.10)
            
            results.append({
                'hashtag': tag,
                'category': category,
                'engagement_score': self._calculate_engagement_score(l, c, s),
                'trending_score': 90 - (i * 8),
                'post_count': random.randint(10, 50),
                'total_engagement': base,
                'avg_engagement': float(base),
                'likes': l,
                'comments': c,
                'shares': s,
                'avg_likes': float(l),
                'avg_comments': float(c),
                'avg_shares': float(s),
                'sentiment': 'positive',
                'sentiment_score': 0.6,
                'hashtag_url': f"https://www.facebook.com/hashtag/{tag}",
                'is_estimated': True
            })
        
        return results
    
