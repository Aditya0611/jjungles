# Code Logic and Architecture Documentation

## Overview

Your Instagram scraper is a **comprehensive trending hashtag discovery and analysis system** that:
1. Discovers trending hashtags from Instagram's Explore page
2. Extracts real engagement metrics from posts
3. Performs sentiment and language analysis
4. Stores normalized data in Supabase
5. Manages trend lifecycle (archiving, decay)

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAIN ENTRY POINT                          â”‚
â”‚                   main() / run_scraper_job()                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single Run    â”‚            â”‚  Scheduled Run  â”‚
â”‚  (--run-once)  â”‚            â”‚  (APScheduler)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   run_scraper_job()   â”‚
            â”‚  (Main Orchestrator)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  1. Setup    â”‚ â”‚  2. Login   â”‚ â”‚  3. Scrape â”‚
â”‚  Browser &   â”‚ â”‚  Instagram  â”‚ â”‚  & Analyze â”‚
â”‚  Supabase    â”‚ â”‚             â”‚ â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  4. Save to Database  â”‚
            â”‚  5. Lifecycle Cleanup â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Detailed Workflow

### Phase 1: Initialization (`run_scraper_job()`)

**Location**: `main.py` line ~3815

**Logic Flow:**
```python
1. Validate configuration (Config.validate())
   â”œâ”€ Check Instagram credentials
   â”œâ”€ Check Supabase credentials
   â””â”€ Validate scraping parameters

2. Generate unique version ID (UUID)
   â””â”€ Tracks this scraper run

3. Setup observability
   â”œâ”€ Set request ID
   â”œâ”€ Start trace
   â””â”€ Initialize metrics

4. Initialize Playwright browser
   â”œâ”€ Configure proxy (if provided)
   â”œâ”€ Set viewport size
   â””â”€ Create browser context

5. Connect to Supabase
   â””â”€ Create Supabase client
```

### Phase 2: Authentication (`login_instagram()`)

**Location**: `main.py` line ~550

**Logic Flow:**
```python
1. Navigate to Instagram login page
   â””â”€ Handle navigation errors with retry

2. Handle cookie consent
   â””â”€ Dismiss cookie banners

3. Find login form elements
   â”œâ”€ Multiple selector strategies for username field
   â”œâ”€ Find password field
   â””â”€ Handle dynamic React components

4. Enter credentials
   â”œâ”€ Human-like typing delays
   â””â”€ Submit form

5. Verify login success
   â”œâ”€ Check for home indicators
   â”œâ”€ Verify URL changes
   â””â”€ Handle verification challenges

6. Dismiss popups
   â””â”€ "Not Now" buttons, notifications

7. Fallback to manual login (if automated fails)
   â””â”€ Wait for user to manually log in
```

**Key Features:**
- Multiple fallback strategies for finding form elements
- Handles React/JavaScript-heavy pages
- Graceful error handling with manual login fallback
- Anti-detection measures (delays, human-like behavior)

### Phase 3: Hashtag Discovery (`discover_trending_hashtags()`)

**Location**: `main.py` line ~2027

**Logic Flow:**
```python
1. Navigate to Instagram Explore page (/explore/)
   â”œâ”€ Handle navigation errors
   â””â”€ Retry logic if navigation fails

2. Wait for page to load
   â”œâ”€ Wait for body element
   â”œâ”€ Wait for React to render
   â””â”€ Handle network idle state

3. Dismiss blocking popups
   â””â”€ "Turn on Notifications", etc.

4. Scroll to load more posts
   â”œâ”€ Scroll SCROLL_COUNT times (default: 15)
   â”œâ”€ Wait between scrolls
   â””â”€ Collect posts as they load

5. Extract post links
   â”œâ”€ Find all post links (a[href*='/p/'], a[href*='/reel/'])
   â”œâ”€ Deduplicate posts
   â””â”€ Store post URLs

6. Extract hashtags from posts
   â”œâ”€ Visit each post (or extract from link previews)
   â”œâ”€ Find hashtag links (a[href*='/explore/tags/'])
   â””â”€ Count hashtag frequency

7. Categorize hashtags
   â””â”€ categorize_hashtag() function
      â”œâ”€ Fashion, Fitness, Food, Travel, etc.
      â””â”€ Returns category name

8. Select top hashtags
   â”œâ”€ Filter by minimum frequency
   â”œâ”€ Sort by frequency
   â””â”€ Take top N hashtags (TOP_HASHTAGS_TO_SAVE)

9. Build hashtag data structure
   â”œâ”€ hashtag name
   â”œâ”€ frequency count
   â”œâ”€ sample posts (for engagement analysis)
   â””â”€ category
```

**Key Features:**
- Handles dynamic content loading
- Multiple strategies for finding posts
- Frequency-based ranking
- Automatic categorization

### Phase 4: Engagement Analysis (`analyze_hashtag_engagement()`)

**Location**: `main.py` line ~2610

**Logic Flow:**
```python
For each hashtag:
    1. Get sample posts (default: 3 posts per hashtag)
    
    2. For each sample post:
        a. Call get_post_engagement(post_url)
           â”œâ”€ Navigate to post
           â”œâ”€ Extract likes
           â”œâ”€ Extract comments
           â”œâ”€ Extract views (for videos/reels)
           â”œâ”€ Extract caption
           â”œâ”€ Detect content type (photo/reel/video)
           â”œâ”€ Detect language (langdetect)
           â””â”€ Analyze sentiment (TextBlob + VADER)
        
        b. Collect engagement data
    
    3. Aggregate across all posts:
        â”œâ”€ Calculate averages (avg_likes, avg_comments, avg_engagement)
        â”œâ”€ Aggregate sentiment (positive/neutral/negative counts)
        â”œâ”€ Aggregate language (distribution, primary language)
        â”œâ”€ Aggregate content types (photo/reel/video distribution)
        â””â”€ Calculate total engagement
    
    4. Return aggregated summary
```

**Key Features:**
- Real engagement metrics extraction
- Multi-post aggregation
- Sentiment analysis
- Language detection
- Content type distribution

### Phase 5: Engagement Extraction (`get_post_engagement()`)

**Location**: `main.py` line ~1726

**Logic Flow:**
```python
1. Navigate to post URL
   â””â”€ Handle navigation errors

2. Detect content type
   â”œâ”€ Check URL for /reel/ or /p/
   â”œâ”€ Check for video indicators
   â””â”€ Return: 'photo', 'reel', 'video', 'carousel'

3. Extract views (if video/reel)
   â”œâ”€ Multiple selector strategies
   â”œâ”€ Parse K/M suffixes (5.2K â†’ 5200, 1.2M â†’ 1200000)
   â””â”€ Handle different Instagram formats

4. Extract likes
   â”œâ”€ Multiple selector strategies
   â”œâ”€ Parse K/M suffixes
   â””â”€ Fallback to element counting

5. Extract comments
   â”œâ”€ Multiple selector strategies
   â”œâ”€ Parse K/M suffixes
   â””â”€ Fallback to counting visible elements

6. Extract caption
   â”œâ”€ Multiple selector strategies
   â”œâ”€ Clean hashtags and mentions
   â””â”€ Limit length

7. Analyze caption
   â”œâ”€ Language detection (langdetect)
   â”‚  â”œâ”€ ISO 639-1 code
   â”‚  â”œâ”€ Confidence score
   â”‚  â””â”€ All detected languages
   â””â”€ Sentiment analysis
      â”œâ”€ TextBlob polarity
      â”œâ”€ VADER sentiment (if available)
      â””â”€ Combined score

8. Return engagement data structure
```

**Key Features:**
- Multiple extraction strategies (robust to UI changes)
- Handles K/M number suffixes
- Fallback values if extraction fails
- Comprehensive metadata collection

### Phase 6: Data Normalization (`create_trend_record()`)

**Location**: `main.py` line ~2900+

**Logic Flow:**
```python
1. Take aggregated engagement data
2. Create TrendRecord dataclass:
   â”œâ”€ platform: "Instagram"
   â”œâ”€ url: hashtag URL
   â”œâ”€ hashtags: [list of hashtags]
   â”œâ”€ likes: average likes
   â”œâ”€ comments: average comments
   â”œâ”€ views: average views
   â”œâ”€ language: primary language (ISO 639-1)
   â”œâ”€ timestamp: current time
   â”œâ”€ engagement_score: avg_engagement
   â”œâ”€ version: VERSION_ID
   â””â”€ raw_blob: {
       â”œâ”€ sentiment_summary
       â”œâ”€ language_summary
       â”œâ”€ content_types
       â””â”€ all aggregated data
   }
```

### Phase 7: Database Storage (`save_trends_to_database()`)

**Location**: `main.py` line ~3650+

**Logic Flow:**
```python
1. For each hashtag:
   â”œâ”€ Analyze engagement (analyze_hashtag_engagement)
   â”œâ”€ Create TrendRecord
   â””â”€ Add to batch

2. Bulk insert to Supabase
   â”œâ”€ Check existing records
   â”œâ”€ Separate new inserts vs updates
   â”œâ”€ Bulk insert new records
   â”‚  â”œâ”€ Retry with exponential backoff
   â”‚  â””â”€ Fallback to individual inserts
   â””â”€ Update existing records individually

3. Handle errors
   â”œâ”€ Track success/failure counts
   â””â”€ Log errors with error codes
```

**Key Features:**
- Bulk operations for efficiency
- Retry logic with exponential backoff
- Fallback to individual inserts
- Lifecycle tracking (first_seen, last_seen)

### Phase 8: Lifecycle Management (`cleanup_old_trends()`)

**Location**: `main.py` line ~3290+

**Logic Flow:**
```python
1. Get all trends from database

2. For each trend:
   â”œâ”€ Calculate days since last seen
   â”œâ”€ Apply engagement score decay (if inactive > 14 days)
   â”‚  â””â”€ Decay formula: score * (1 - decay_rate) ^ weeks_inactive
   â””â”€ Archive or delete (if older than 30 days)

3. Update database
   â””â”€ Save decayed scores, archive status
```

## Key Data Structures

### TrendRecord
```python
@dataclass
class TrendRecord:
    platform: str              # "Instagram"
    url: str                   # Hashtag URL
    hashtags: List[str]        # List of hashtags
    likes: int                 # Average likes
    comments: int              # Average comments
    views: int                 # Average views
    language: str              # ISO 639-1 code
    timestamp: datetime        # Scrape timestamp
    engagement_score: float    # Calculated score
    version: str               # Version ID
    raw_blob: Dict[str, Any]   # All aggregated data
```

### Engagement Data
```python
{
    'likes': 1234,
    'comments': 56,
    'views': 50000,           # For videos/reels
    'total_engagement': 1290, # likes + comments
    'is_video': True,
    'format': 'reel',
    'caption': 'Post caption text',
    'language': 'en',
    'language_confidence': 0.95,
    'language_detected': True,
    'sentiment': {
        'polarity': 0.15,
        'label': 'positive',
        'emoji': 'ðŸ˜Š',
        'combined_score': 0.2
    }
}
```

### Hashtag Data
```python
{
    'hashtag': 'trending',
    'frequency': 5,           # Times seen across posts
    'posts_count': 10,        # Total posts using hashtag
    'sample_posts': [        # Post URLs for analysis
        '/p/ABC123/',
        '/p/DEF456/',
        '/p/GHI789/'
    ],
    'category': 'fashion'    # Auto-categorized
}
```

## Key Algorithms

### 1. Engagement Score Calculation

```python
# Per post:
total_engagement = likes + comments

# Per hashtag (trend):
engagement_score = average(total_engagement across all sample posts)
```

**Example:**
- Post 1: 1000 likes + 50 comments = 1050 engagement
- Post 2: 2000 likes + 100 comments = 2100 engagement
- Post 3: 1500 likes + 75 comments = 1575 engagement
- **Trend Score**: (1050 + 2100 + 1575) / 3 = **1575.0**

### 2. Hashtag Frequency Analysis

```python
1. Collect all posts from Explore page
2. Extract hashtags from each post
3. Count occurrences: hashtag_counter[hashtag] += 1
4. Filter: frequency >= MIN_HASHTAG_FREQUENCY
5. Sort by frequency (descending)
6. Take top N: TOP_HASHTAGS_TO_SAVE
```

### 3. Sentiment Aggregation

```python
For each post:
    sentiment = analyze_sentiment(caption)
    # Returns: {'polarity': 0.15, 'label': 'positive', ...}

Aggregate:
    sentiment_counts = {
        'positive': count of positive posts,
        'neutral': count of neutral posts,
        'negative': count of negative posts
    }
    avg_polarity = average(all polarities)
    overall_label = most_common label
```

### 4. Language Distribution

```python
For each post:
    language_info = detect_language(caption)
    # Returns: {'language': 'en', 'confidence': 0.95, ...}

Aggregate:
    language_distribution = Counter()
    # Count: {'en': 17, 'es': 3}
    
    primary_language = most_common language
    primary_language_percent = (count / total) * 100
    avg_confidence = average confidence for primary language
```

### 5. Engagement Score Decay

```python
days_inactive = (now - last_seen).days
weeks_inactive = days_inactive / 7

if days_inactive > 14:  # TREND_INACTIVE_DAYS
    decay_rate = 0.05  # 5% per week (TREND_DECAY_RATE)
    decayed_score = original_score * ((1 - decay_rate) ** weeks_inactive)
    min_score = original_score * 0.1  # Never go below 10%
    decayed_score = max(decayed_score, min_score)
```

## Error Handling Strategy

### 1. Navigation Errors
- **Retry logic**: Try navigation up to 2 times
- **Fallback**: Check if already on correct page
- **Error codes**: `SCRAPE_NAVIGATION_FAILED`

### 2. Element Not Found
- **Multiple selectors**: Try different CSS selectors
- **Fallback values**: Use estimated values if extraction fails
- **Error codes**: `SCRAPE_ELEMENT_NOT_FOUND`

### 3. Proxy Failures
- **Error detection**: Check for proxy-related errors
- **Error codes**: `PROXY_CONNECTION_FAILED`, `PROXY_TIMEOUT`
- **Metrics**: Track proxy failures

### 4. Database Errors
- **Retry with backoff**: Exponential backoff (2s, 4s, 6s)
- **Fallback**: Individual inserts if bulk fails
- **Error codes**: `DB_INSERT_ERROR`, `DB_CONNECTION_ERROR`

## Observability Integration

### Structured Logging
- **JSON format**: All logs in structured JSON
- **Request/Trace IDs**: Track operations end-to-end
- **Error codes**: Categorized error taxonomy

### Metrics Collection
- **Counters**: Request counts, error counts, job counts
- **Histograms**: Request duration, operation duration
- **Labels**: Platform, adapter, outcome, error type

### Event Tracing
- **Start trace**: `logger.start_trace('operation_name')`
- **End trace**: `logger.end_trace('operation_name', success=True)`
- **Duration tracking**: Automatic duration calculation

## Configuration System

### Environment Variables
```python
# Credentials
INSTAGRAM_USERNAME
INSTAGRAM_PASSWORD
SUPABASE_URL
SUPABASE_KEY

# Scraping Parameters
SCROLL_COUNT = 15              # Times to scroll Explore page
POSTS_TO_SCAN = 400            # Max posts to analyze
MIN_HASHTAG_FREQUENCY = 1      # Min occurrences to include
TOP_HASHTAGS_TO_SAVE = 10      # Top N hashtags to save
POSTS_PER_HASHTAG = 3          # Sample posts per hashtag

# Proxy
PROXY_SERVER
PROXY_USERNAME
PROXY_PASSWORD

# Language
ENABLE_LANGUAGE_DETECTION = true
FILTER_LANGUAGES = "en,es,fr"  # Comma-separated
MIN_LANGUAGE_CONFIDENCE = 0.5

# Scheduling
SCHEDULE_HOURS = 3             # Run every N hours

# Lifecycle
TREND_EXPIRATION_DAYS = 30
TREND_INACTIVE_DAYS = 14
TREND_DECAY_ENABLED = true
TREND_DECAY_RATE = 0.05

# Observability
USE_JSON_LOGGING = true
LOG_LEVEL = INFO
```

## Scheduling System

### APScheduler Integration
```python
scheduler = BlockingScheduler()
scheduler.add_job(
    run_scraper_job,
    trigger=CronTrigger(hour=f'*/{SCHEDULE_HOURS}'),
    id='instagram_scraper_job'
)
scheduler.start()
```

**Modes:**
- **Scheduled**: Runs every N hours automatically
- **Single Run**: `python main.py --run-once` for testing

## Data Flow Summary

```
1. Start Job
   â†“
2. Login to Instagram
   â†“
3. Navigate to /explore/
   â†“
4. Scroll & Collect Posts
   â†“
5. Extract Hashtags & Count Frequency
   â†“
6. Select Top Hashtags
   â†“
7. For Each Hashtag:
   â”œâ”€ Visit Sample Posts
   â”œâ”€ Extract Engagement Metrics
   â”œâ”€ Analyze Sentiment
   â”œâ”€ Detect Language
   â””â”€ Aggregate Data
   â†“
8. Create TrendRecord Objects
   â†“
9. Bulk Insert to Supabase
   â†“
10. Lifecycle Cleanup (Decay, Archive)
    â†“
11. Export to JSON/CSV
    â†“
12. End Job
```

## Key Design Patterns

### 1. Adapter Pattern (Potential)
- Base scraper interface (for future multi-platform)
- Platform-specific implementations

### 2. Retry Pattern
- Exponential backoff
- Maximum retry attempts
- Fallback strategies

### 3. Fallback Pattern
- Multiple selector strategies
- Estimated values if extraction fails
- Graceful degradation

### 4. Observer Pattern (Observability)
- Metrics collection
- Event tracing
- Structured logging

### 5. Strategy Pattern
- Multiple extraction strategies
- Platform-specific strategies
- Configurable strategies

## Performance Optimizations

1. **Bulk Operations**: Batch database inserts
2. **Parallel Processing**: Could parallelize post analysis (future)
3. **Caching**: Cache language detection results
4. **Lazy Loading**: Only load what's needed
5. **Connection Pooling**: Reuse Supabase connections

## Security & Compliance

1. **No PII Collection**: Only public content
2. **Rate Limiting**: Delays between requests
3. **User Agent**: Appropriate user agents
4. **Error Handling**: Don't expose sensitive data in errors
5. **Environment Variables**: Credentials in env vars, not code

## Future Extension Points

1. **Multi-Platform**: Add TikTok, Twitter adapters
2. **Proxy Rotation**: Implement proxy pool manager
3. **Async Processing**: Use async/await for parallel scraping
4. **Caching Layer**: Redis for frequently accessed data
5. **API Endpoints**: REST API for querying trends
6. **Real-time Updates**: WebSocket for live trend updates

## Summary

Your code implements a **complete trending hashtag discovery and analysis pipeline**:

1. **Discovery**: Finds trending hashtags from Explore page
2. **Analysis**: Extracts real engagement metrics
3. **Enrichment**: Adds sentiment and language analysis
4. **Storage**: Saves to Supabase with lifecycle management
5. **Observability**: Comprehensive logging and metrics
6. **Automation**: Scheduled runs with APScheduler

The system is **robust** (multiple fallbacks), **observable** (structured logging, metrics), and **maintainable** (clear separation of concerns, error handling).

