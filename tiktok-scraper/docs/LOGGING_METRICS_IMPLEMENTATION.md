# Structured Logging, Event Tracing, Error Taxonomy & Metrics Implementation

## ‚úÖ Implementation Complete

Comprehensive structured logging, event tracing, error taxonomy, and Prometheus-style metrics have been implemented.

## üì¶ Files Created

### `logging_metrics.py`
Complete implementation of:
- **Structured JSON Logging**: JSON formatter for all log entries
- **Event Tracing**: Request/span tracing with correlation IDs
- **Error Taxonomy**: Automatic error classification system
- **Prometheus-style Metrics**: Counters, gauges, and histograms

## üéØ Features Implemented

### 1. Structured JSON Logging
- JSON format for all log entries
- Automatic inclusion of trace IDs, request IDs, span IDs
- Error information with taxonomy
- Duration tracking
- Configurable via `USE_JSON_LOGGING` environment variable

**Example JSON Log:**
```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "INFO",
  "logger": "base",
  "message": "Scraping completed successfully",
  "trace_id": "abc-123-def",
  "request_id": "req-456",
  "span_id": "span-789",
  "duration_ms": 1234.5,
  "hashtags_count": 25
}
```

### 2. Event Tracing Per Request
- **TraceContext**: Main request tracing context
- **Span**: Individual operation spans
- Automatic correlation ID propagation
- Duration tracking for all operations
- Nested span support

**Usage:**
```python
with trace_request("scrape_operation") as trace:
    with trace.create_span("navigate_page") as span:
        # Operation code
        pass
```

### 3. Error Taxonomy
- **ErrorCategory**: Network, Timeout, Authentication, Parsing, Validation, Rate Limit, Proxy, Database, Unknown
- **ErrorSeverity**: Low, Medium, High, Critical
- **ErrorInfo**: Structured error information
- Automatic error classification
- Error counter metrics by category/severity

**Error Classification:**
- Network errors ‚Üí `ErrorCategory.NETWORK`
- Proxy failures ‚Üí `ErrorCategory.PROXY`
- Authentication failures ‚Üí `ErrorCategory.AUTHENTICATION`
- Timeouts ‚Üí `ErrorCategory.TIMEOUT`
- Database errors ‚Üí `ErrorCategory.DATABASE`
- Rate limiting ‚Üí `ErrorCategory.RATE_LIMIT`

### 4. Prometheus-Style Metrics

#### Counters
- `scrape_attempts_total` - Total scrape attempts (with `use_proxy` label)
- `scrape_attempts_success_total` - Successful attempts
- `scrape_attempts_failed_total` - Failed attempts
- `scrape_attempts_insufficient_data_total` - Insufficient data attempts
- `scraper_runs_total` - Total scraper runs (with `platform`, `region` labels)
- `scraper_runs_success_total` - Successful runs
- `scraper_runs_failed_total` - Failed runs
- `errors_total` - Total errors (with `category`, `severity`, `error_type` labels)
- `hashtag_parse_errors_total` - Hashtag parsing errors
- `database_uploads_success_total` - Successful database uploads
- `database_uploads_failed_total` - Failed database uploads

#### Gauges
- `scrape_requests_active` - Currently active scrape requests
- `scrape_hashtags_scraped` - Number of hashtags scraped
- `database_hashtags_uploaded` - Number of hashtags uploaded
- `scraper_last_run_duration_seconds` - Last run duration

#### Histograms
- `scrape_attempt_duration_ms` - Attempt duration (with `attempt`, `use_proxy`, `failed` labels)
- `scraper_run_duration_seconds` - Run duration (with `platform`, `region` labels)
- `database_upload_duration_ms` - Database upload duration

## üîß Integration Points

### In `base.py`:
1. **Logging Setup**: Replaced basic logging with JSON structured logging
2. **Tracing**: Added `trace_request` and spans to:
   - `run_scraper()` - Main entry point
   - `scrape_tiktok_hashtags()` - Main scraping function
   - `scrape_single_attempt()` - Single attempt with spans
3. **Error Handling**: All exceptions use `log_error()` with taxonomy
4. **Metrics**: Metrics collection throughout:
   - Attempt counters
   - Success/failure tracking
   - Duration histograms
   - Active request gauges

## üìä Metrics Export

### Functions:
- `get_metrics_summary()` - Get all metrics in Prometheus format
- `print_metrics_summary()` - Print metrics to console

### Metrics Format:
```python
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "metrics": {
    "counters": {
      "scrape_attempts_total:use_proxy=False": 5,
      "scrape_attempts_total:use_proxy=True": 2,
      "errors_total:category=network,severity=medium": 1
    },
    "gauges": {
      "scrape_requests_active": 0.0,
      "scrape_hashtags_scraped": 25.0
    },
    "histograms": {
      "scrape_attempt_duration_ms": {
        "count": 7,
        "sum": 35000.0,
        "avg": 5000.0,
        "min": 2000.0,
        "max": 8000.0
      }
    }
  }
}
```

## üöÄ Usage

### Enable JSON Logging
```bash
export USE_JSON_LOGGING=true
python base.py
```

### Access Metrics
```python
from logging_metrics import metrics, get_metrics_summary

# Get all metrics
summary = get_metrics_summary()
print(summary)

# Get specific counter
attempts = metrics.get_counter("scrape_attempts_total", labels={"use_proxy": "true"})

# Get histogram stats
duration_stats = metrics.get_histogram_stats("scrape_attempt_duration_ms")
```

## üìù Log Examples

### Success Log (JSON):
```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "INFO",
  "message": "Scraping completed successfully",
  "trace_id": "abc-123",
  "request_id": "req-456",
  "duration_ms": 12345.67,
  "hashtags_count": 25,
  "platform": "TikTok",
  "region": "en"
}
```

### Error Log (JSON):
```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "ERROR",
  "message": "Error [proxy/medium]: Proxy connection failed",
  "trace_id": "abc-123",
  "request_id": "req-456",
  "error_info": {
    "category": "proxy",
    "severity": "medium",
    "error_type": "PlaywrightError",
    "message": "Proxy connection failed",
    "details": {"url": "...", "use_proxy": true}
  }
}
```

## üéØ Benefits

1. **Observability**: Complete visibility into scraper operations
2. **Debugging**: Trace IDs enable request correlation
3. **Monitoring**: Prometheus-style metrics ready for dashboards
4. **Error Analysis**: Taxonomy enables error pattern analysis
5. **Performance**: Duration tracking for optimization

## üìã Environment Variables

- `USE_JSON_LOGGING` - Enable JSON logging (default: true)
- All existing environment variables still work

## üîç Next Steps

1. Integrate with monitoring systems (Prometheus, Grafana)
2. Set up log aggregation (ELK, Splunk, etc.)
3. Create dashboards from metrics
4. Set up alerts based on error taxonomy

---

**Status**: ‚úÖ Complete
**Files**: `logging_metrics.py`, `base.py` (updated)

